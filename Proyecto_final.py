# -*- coding: utf-8 -*-
"""Copia de Proyecto-Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tWO-jqjMEPZWhPiGmrkyZSpToRIvh-Vw

### **Proyecto de Fundamentos Analítica I** *(Daniel Delgado Caicedo, Raúl Alberto Echeverry López, Luis Esteban Ordoñez Erazo, Fabian Salazar Figueroa)*

TABLA DE CONTENIDO

1. Introducción

2. Contexto
    - Pregunta inteligente (SMART)
        - Específico (Specific)
        - Medible (Measurable)
        - Alcanzable (Achievable)
        - Relevante (Relevant)
        - Temporal (Time-bound)  
    - Objetivo
    - Diccionario de Datos
    - Convenciones

3. Análisis exploratorio (EDA)
    - Importación de bibliotecas
    - Carga de datos (dataset, dataframe, base de datos, etc)
    - Exploración del conjunto de datos
        - Hallazgos
    - Tipos de Datos
        - Análisis de columnas o headers
        - Análisis variables categóricas y de tipo de datos Objeto
        - Análisis de datos faltantes o nulos
    - Descripcion estadistica
        - Análisis de outliers
        - Histogramas de outliers médicos
    - Visualización de datos
    - Analisis univariado y multivariado
    - Conclusiones

4. Modelos

5. Análisis de Componentes Principales (PCA)

6. Clusteging
  - K-Means Cluster
  - Cluster Jerárquico (Ward)

5. Bibliografia

# 1. Introducción

# **Síndrome del ovario poliquístico (PCOS)**

El síndrome de ovario poliquístico (PCOS) es un problema hormonal que afecta a las mujeres en la edad reproductiva. Aquí se presenta información relevante sobre el PCOS:

Síntomas y características del PCOS:
- **Períodos menstruales irregulares**: Las mujeres con PCOS pueden experimentar ciclos menstruales irregulares o ausencia de menstruación.
- **Exceso de vello corporal (hirsutismo)**: El PCOS puede causar un aumento en el vello facial, en el pecho, el abdomen y la espalda.
- **Acné**: Las alteraciones hormonales pueden provocar brotes de acné.
- **Calvicie**: Algunas mujeres con PCOS pueden experimentar adelgazamiento del cabello.
- **Dificultad para quedar embarazada**: El PCOS es una causa común de infertilidad.

Causas y diagnóstico:
El PCOS está relacionado con un desequilibrio hormonal y problemas metabólicos.
El diagnóstico se basa en la presencia de síntomas, análisis de sangre y ecografía para evaluar los ovarios.

Tratamiento:
El tratamiento puede incluir cambios en el estilo de vida (dieta y ejercicio), medicamentos para regular los ciclos menstruales y mejorar la fertilidad, y en algunos casos, cirugía.

# 2. Contexto

# **Pregunta inteligente (SMART)**

Que diferencias relevantes y patognomónicas existen entre las mujeres diagnosticadas con sindrome de ovario poliquistico y las que no presentan el síndrome en los 10 hospitales de Kerala, India?

- **Específico (Specific)**: Se busca saber si hay diferencias importantes entre las mujeres pacientes que se hicieron el dianostico para sindrome de ovario poliquistico en los 10 hospitales.
- **Medible (Measurable)**: A través de modelos estadísticos
- **Alcanzable (Achievable)**: La información y estudios previos permiten catalogarlo como alcanzable
- **Relevante (Relevant)**: Permitirá ayudar a comprender más y mejor el síndrome de ovario poliquístico.
- **Temporal (Time-bound)**: Se pretende estudiarlo durante 2 años

# **Objetivo**

Evaluar si se evidencian diferencias patognomónicas de acuerdo a la información recopilada en la base de datos sobre las pacientes que se hicieron el diagnóstico para el síndrome de ovario poliquistico (PCOS).

# **Diccionario de Datos**

- (s-n):	Contador de pacientes
- paciente-id:	Registro y asignación de id único y anónimo a cada paciente
- pcos(s-n):	Resultado (Si = 1, No = 0) del examen de síndrome de ovario poliquístico de la paciente
- edad-a:	Edad en años de la paciente al momento de diagnosticarse para pcos
- peso-kg:	Peso en Kilogramos de la paciente al momento de diagnosticarse para pcos
- estatura-cm:	Estatura en centimentro de la paciente al momento de diagnosticarse para pcos
- imc:	Índice de masa corporal de la paciente al momento de diagnosticarse para pcos
- grupo-sanguineo:	Factor sanguíneo RH de la paciente
- frecuencia-cardiaca-bpm:	Frecuencia cardiaca medida en pulso por minuto de la paciente al momento de diagnosticarse para pcos
- frecuencia-respiratoria-respiraciones/min:	Frecuencia respiratoria medida en respiraciones por minuto de la paciente al momento de diagnosticarse para pcos
- hemoglobina-g/dl:	Hemoglobina de la paciente expresada en gramos por decilitro al momento de diagnosticarse para pcos
- ciclo-r/i:	Duración del flujo menstrual de la paciente
- duracion-ciclo-d:	Duración del ciclo mesntrual de la paciente medido en dias
- tiempo-casada-a:	Tiempo de casada de la paciente medido en años al momento de diagnosticarse para pcos
- embarazada(s-n):	Resultado de prueba de embarazo (Si = 1, No = 0) de la paciente al momento de diagnosticarse para pcos
- nro-abortos:	Número de abortos que ha sufrido la paciente hasta el momento de diagnosticarse para pcos
- h-beta-hcg-I-mIU/mL:	Resultado de la prueba de la hormona gonadotropina coriónica humana (hCG) de la paciente al momento de diagnosticarse para pcos
- h-beta-hcg-II-mIU/mL:	Resultado de la segunda prueba de la hormona gonadotropina coriónica humana (hCG) de la paciente al momento de diagnosticarse para pcos
- h-fsh-mIU/mL:	Resultado de la prueba de la hormona foliculoestimulante (FSH) de la paciente al momento de diagnosticarse para pcos
- h-lh-mIU/mL:	Resultado de la prueba de la hormona luteinizante (LH) de la paciente al momento de diagnosticarse para pcos
- h-fsh/h-lh:	Resultado del índice del cociente entre La hormona foliculoestimulante (FSH) y la hormona luteinizante (LH) de la paciente al momento de diagnosticarse pcos
- cadera-pulg:	Medida de la cadera de la paciente en pulgadas al momento de diagnosticarse pcos
- cintura-pulg:	Medida de la cintura de la paciente en pulgadas al momento de diagnosticarse pcos
- ind-cintura/cadera:	Resultado del índice del cociente entre la medida de la cintura y la medida de la cadera de la paciente al momento de diagnosticarse pcos
- h-tsh-mIU/L:	Resultado de la prueba de la hormona estimulante de la tiroides (TSH) de la paciente al momento de diagnosticarse pcos
- h-amh-ng/mL:	Resultado de la prueba de la hormona antimülleriana (AMH) de la paciente al momento de diagnosticarse pcos
- h-prl-ng/mL:	Resultado de la hormona prolactina (PRL) de la paciente al momento de diagnosticarse pcos
- ex-vit-d3-ng/mL:	Resultado del examen de la vitamina D de la paciente al momento de diagnosticarse pcos
- h-prg-ng/mL:	Resultado de la hormona progesterona (PRG) de la paciente al momento de diagnosticarse pcos
- ex-rbs-mg/dl:	Resultado del examen del análisis de glucosa en sangre al azar (RBS) de la paciente al momento de diagnosticarse pcos
- ganancia-peso(s-n):	Respuesta (Si = 1, No = 0) de la paciente comparando su peso al momento de diagnosticarse pcos contra el control médico inmediantamente anterior
- crecimiento-cabello(s-n):	Respuesta (Si = 1, No = 0) de la paciente comparando su largo de cabello al momento de diagnosticarse pcos contra el control médico inmediantamente anterior
- oscurecimiento-piel(s-n):	Respuesta (Si = 1, No = 0) de la paciente comparando si hay oscurecimeinto de su piel al momento de diagnosticarse pcos contra el control médico inmediantamente anterior
- perdida-cabello(s-n):	Respuesta (Si = 1, No = 0) de la paciente comparando si hay pérdida de cabello al momento de diagnosticarse pcos contra el control médico inmediantamente anterior
- barro-espinilla(s-n):	Respuesta (Si = 1, No = 0) de la paciente comparando si hay aumento o aparación de espinillas, barros o granos en la piel al momento de diagnosticarse pcos contra el control médico inmediantamente anterior
- comida-rapida(s-n):	Respuesta (Si = 1, No = 0) de la paciente sobre  su alimentación con comidas rapidas o "chatarras"
- ejercicio-regular(s-n):	Respuesta (Si = 1, No = 0) de la paciente sobre  su ejercitación regular
- ps-sistolica-mmHg:	Resultado de la presión sanguinea sistólica de la paciente al momento de diagnosticarse pcos
- ps-diastolica-mmHg:	Resultado de la presión sanguinea diastólica de la paciente al momento de diagnosticarse pcos
- nro-foliculos-ovario-izq:	Número de folículos antrales en el ovario izquierdo de la paciente al momento de diagnosticarse pcos
- nro-foliculos-ovario-der:	Número de folículos antrales en el ovario derecho de la paciente al momento de diagnosticarse pcos
- prom-tam-foliculos-ovario-izq-mm:	Tamaño promedio de los folículos antrales en el ovario izquierdo de la paciente al momento de diagnosticarse pcos
- prom-tam-foliculos-ovario-der-mm:	Tamaño promedio de los folículos antrales en el ovario derecho de la paciente al momento de diagnosticarse pcos
- endometrio-mm:	Tamaño del endometrio de la paciente al momento de diagnosticarse pcos medido en milimetros

# **Convenciones**

1. **pcos** significa síndrome de ovario poliquistico
2. Por cada pregunta de respuesta tipo si-no (si = 1, no = 0), se identifica en el encabezado de cada parametro al final del nombre del mismo así: (s-n)
3. Grupo sanguíneo se define así:
(A+ = 11, A- = 12, B+ = 13, B-= 14, O+ = 15, O- = 16, AB+ = 17, AB- = 18)
4. Presión sanguínea ingresada como sistólica y diastólica de forma separada
5. Casos de la hormona Beta-HCG son mensionados como **h-beta-hcg-I-mIU/mL** y **h-beta-hcg-II-mIU/mL**
6. El prefijo **h** anterior al primer **"-"** en el nombre de cada parámetro significa hormona. Ejemplo: **h**-beta-hcg-I-mIU/mL, donde ese prefijo **h** significa hormona.
7. El prefijo **ex** anterior al primer **"-"** en el nombre de cada parámetro significa examen
8. Las unidades de medida estarán posteriores al último **"-"** en el nombre de cada parámetro y respetarán la nomenclatura médica. Ejemplo: h-beta-hcg-I-**mIU/mL**, donde su unidad de medida es: **mIU/mL**
9. Cada fila representa la información y resultados de cada paciente en el proceso de diagnosticarse para pcos

## *Dataset tomado de:*
author = {Prasoon Kottarathil},
title = {Polycystic ovary syndrome (PCOS)},
year = {2020},
publisher = {kaggle},
journal = {Kaggle Dataset},
how published = {\url{https://www.kaggle.com/prasoonkottarathil/polycystic-ovary-syndrome-pcos}}

El **Dataset** contiene todos los **parámetros físicos y clínicos** para determinar problemas relacionados con el **síndrome de ovario poliquístico (PCOS)** y la **infertilidad**. Los datos se recopilaron en **10 hospitales diferentes en Kerala, India**.

# 3. Análisis exploratorio

**Importación de bibliotecas**
"""

# Importacion de pandas y asignándole el alias pd, se usa para el tratamiento de los datos
import pandas as pd
# Importacion de numpy y asignándole el alias np, se usa para operaciones matriciales y con vectores
import numpy as np
# Importacion de matplotlib.pyplot y asignándole el alias plt, se usa para graficar
import matplotlib.pyplot as plt
# Importacion de plotly.express y asignándole el alias px, se usa para graficar
import plotly.express as px
# Importacion de seaborn y asignándole el alias sns, se usa para graficar junto a matplotlib y pandas
import seaborn as sns
# Importacion de enable_iterative_imputer de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para poder importar iterativeimputer
from sklearn.experimental import enable_iterative_imputer
# Importacion de iterativeimputer de la biblioteca sklearn, se usa para imputar datos faltantes
from sklearn.impute import IterativeImputer
# importa la clase Pipeline de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para construir flujos de trabajo
from sklearn.pipeline import Pipeline
# importa la clase StandardScaler de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para estandarizar las caracteristicas de los datos
from sklearn.preprocessing import StandardScaler
# importa la clase PCA de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para reducir dimension de los datos y encontrar caracteristicas más importantes
from sklearn.decomposition import PCA
# importa la clase LogisticRegression de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una regresión logística, herramietna para clasificacion
from sklearn.linear_model import LogisticRegression
# importa la clase KMeans de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una imputacion por clustering
from sklearn.cluster import KMeans
# importa la clase SimpleImputer de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una imputacion por clustering
from sklearn.impute import SimpleImputer
# importa la clase train_test_split de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.model_selection import train_test_split
# importa la clase RandomForestClassifier de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.ensemble import RandomForestClassifier
# importa la clase accuracy_score de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.metrics import accuracy_score
# importa la clase cross_val_score de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.model_selection import cross_val_score
# importa la clase GridSearchCV de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.model_selection import GridSearchCV
# importa la clase confusion_matrix de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.metrics import confusion_matrix
# importa la clase classification_report de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.metrics import classification_report

"""**Carga de la base de datos de los pacientes**"""

# Ruta al archivo CSV (local)
#ruta_archivo = r"C:\Users\alfa7\OneDrive\Documentos\PCOS_data.csv"

# Carga los datos desde el archivo CSV su separador de columnas es una coma (,)
df = pd.read_csv('https://raw.githubusercontent.com/alfa7g7/Fundamentos-Analitica-I/main/Data/PCOS_data.csv', sep = ',')

"""**Exploración del conjunto de datos**"""

# Muestra en una tupla la cantidad de (filas, columnas) del DataFrame df
df.shape

# Muestra la información del DataFrame df, donde evidenciaremos el índice, las columnas, el tipo de dato y valores no nulos
print(df.info())

# Observamos las primeras filas del dataframe
df.head()

#Observamos las últimas filas del dataframe
df.tail()

# Muestra el nombre de las columnas (llamdas comunmente headers) del DataFrame df
print(df.columns)

"""- **Hallazgos**

    - df.shape. Nos permitió saber que nuestro dataframe tiene 44 Columnas y 541 filas
    - print(df.info()). Nos indica que nuestro dataframe tiene:
        - 19 columnas con tipo de datos flotante
        - 23 columnas con tipo de dato entero
        - 2 columnas con tipo de datos objeto
        - Determinar que los headers o nombres de cada columnas no están homogéneos, presentan espacios, etc
        - Las variables 'Marraige Status (Yrs)' y 'Fast food (Y/N)' tienen 1 dato faltante o nulo cada una.
    - print(df.columns). Nos permite observar los headers y su nombre. ( Esto debido a que debía analizarse más estos valores por lo observado con la anterior función (print(df.info())))

    Con esta información podemos determinar que se debe estandarizar las columnas o header y adicional procesar o trabajar estas columnas o variables categóricas para que podamos más adelante usar correctamente los modelos estadisticos.

**Tipos de datos**

- **Análisis de columnas o headers**
"""

# Definimos los nuevos nombres de las columnas o headers en un arreglo llamado headers
headers = ['(s-n)','paciente-id','pcos(s-n)','edad-a','peso-kg','estatura-cm','imc','grupo-sanguineo',
           'frecuencia-cardiaca-bpm','frecuencia-respiratoria-respiraciones/min','hemoglobina-g/dl',
           'ciclo-r/i','duracion-ciclo-d','tiempo-casada-a','embarazada(s-n)','nro-abortos','h-beta-hcg-I-mIU/mL',
           'h-beta-hcg-II-mIU/mL','h-fsh-mIU/mL','h-lh-mIU/mL','h-fsh/h-lh','cadera-pulg','cintura-pulg',
           'ind-cintura/cadera','h-tsh-mIU/L','h-amh-ng/mL','h-prl-ng/mL','ex-vit-d3-ng/mL','h-prg-ng/mL',
           'ex-rbs-mg/dl','ganancia-peso(s-n)','crecimiento-cabello(s-n)','oscurecimiento-piel(s-n)',
           'perdida-cabello(s-n)','barro-espinilla(s-n)','comida-rapida(s-n)','ejercicio-regular(s-n)',
           'ps-sistolica-mmHg','ps-diastolica-mmHg','nro-foliculos-ovario-izq','nro-foliculos-ovario-der',
           'prom-tam-foliculos-ovario-izq-mm','prom-tam-foliculos-ovario-der-mm','endometrio-mm']

# Asigno los nuevos nombres de encabezados o columnas a mi dataframe df
df.columns = headers

# Revisión impresa
print(df.info())

"""- **Análisis variables categóricas**

    En este caso particular solo nos interesan las dos columnas tipo objeto ya que las demás se encuentran con un tipo de dato adecuado aunque en algunas faltén algunos datos.
"""

#Primera Columna tipo objeto
df["h-beta-hcg-II-mIU/mL"].head()

#Segunda columna tipo objeto
df["h-amh-ng/mL"].head()

#Convertiremos los valores categoricos en numericos, ya que han sido numeros guardados como cadenas
#Si hay algun valor que no se puede convertir a numerico se convierte a NaN (Not a number) con error='coerce'
df["h-beta-hcg-II-mIU/mL"] = pd.to_numeric(df["h-beta-hcg-II-mIU/mL"], errors='coerce')
df["h-amh-ng/mL"] = pd.to_numeric(df["h-amh-ng/mL"], errors='coerce')

# verificacion de la conversion
df["h-beta-hcg-II-mIU/mL"].head()

# verificacion de la conversion
df["h-amh-ng/mL"].head()

# Revisión impresa, sin variables tipo objeto
print(df.info())

"""- **Análisis de datos faltantes o nulos**

"""

#Validacion de valores nulos en el dataframe muestra True quiere decir que hay valores nulos.
t = df.isnull().any().any()
print(t)

#verificando valores nulos en los parametros o columnas del dataframe si muestra True quiere decir que esa columna
#tiene valores nulos
r = df.isnull().any()
print(r)

"""Identificamos las siguientes columnas o parámetros con valores nulos:
- tiempo-casada-a
- h-beta-hcg-II-mIU/mL
- h-amh-ng/mL
- comida-rapida(s-n)

**IMPUTACION DE VALORES FALTANTES O NULOS**

La imputación de valores faltantes dio lugar a una comparación entre 2 tipos de imputación al final nos decidimos por la imputación múltiple por ecuaciones encadenadas (MICE) debido a que:

- El algoritmo MICE asume que los datos faltan al azar (MAR), es decir, la falta de un campo se puede explicar por los valores de otras columnas, pero no de esa columna

**MICE**

    Verificación de valores faltantes y llenado de datos faltantes o nulos en los parámetros o columnas del dataframe con imputacion MICE
"""

# Imputacion de datos por MICE
# Inicializa el imputador MICE
mice_imputer = IterativeImputer(max_iter=10, random_state=0)
# Realiza la imputación
df_imputed = mice_imputer.fit_transform(df)
# El resultado es un array NumPy, así que puedes convertirlo de nuevo a DataFrame
df_imputed = pd.DataFrame(df_imputed, columns=df.columns)
#df = df_imputed
print(df_imputed.info())

# Cambio las variables con datos ya imputados solamente al dataframe original para que mis variables no cambien todas a tipo flotante
df['tiempo-casada-a'] = df_imputed['tiempo-casada-a']
df['h-beta-hcg-II-mIU/mL'] = df_imputed['h-beta-hcg-II-mIU/mL']
df['h-amh-ng/mL'] = df_imputed['h-amh-ng/mL']
# redondeamos la siguiente variable imputada para evitarun error en su respuesta.
# (Corrección hecha después de evaluación gráfica inicial)
df_imputed['comida-rapida(s-n)'] = df_imputed['comida-rapida(s-n)'].round()
df['comida-rapida(s-n)'] = df_imputed['comida-rapida(s-n)']

df.info()

df['(s-n)'].nunique()

df['paciente-id'].nunique()

"""Observando la estructura del dataframe procedemos a sacar esas dos variables ya que ambas son identificadores tipo contador para enamurar cada uno de los pacientes del estudio, probablemente usadas incialmente para encriptar o enmascarar los datos sensibles de cada paciente (Nombres o identificacion)."""

df.drop(columns = ['(s-n)', 'paciente-id'], axis =1, inplace = True)

df.info()

#verificando valores nulos en los parametros o columnas del dataframe después de la imputacion por MICE
r = df.isnull().any()
print(r)

#Validacion de que no queden valores nulos en el dataframe Si muestra False quiere decir que ya no hay nulos
t = df.isnull().any().any()
print(t)

"""**Descripción estadística**"""

# Resumen estadístico del dataframe df
df.describe()

"""- **Análisis de outliers**"""

df.max()

# Exportacion del dataframe para creacion de modelados
#df.to_csv("data.csv")

px.box(df)

"""Análisis gráfico variables categóricas"""

# Gráfico para las variables categoricas SI = 1 - NO = 0
px.box(df, y = ['pcos(s-n)', 'embarazada(s-n)', 'ganancia-peso(s-n)', 'crecimiento-cabello(s-n)',
                'oscurecimiento-piel(s-n)', 'perdida-cabello(s-n)', 'barro-espinilla(s-n)',
                'comida-rapida(s-n)', 'ejercicio-regular(s-n)'])

px.histogram(df['pcos(s-n)'])

px.histogram(df['embarazada(s-n)'])

px.histogram(df['crecimiento-cabello(s-n)'])

px.histogram(df['oscurecimiento-piel(s-n)'])

px.histogram(df['perdida-cabello(s-n)'])

px.histogram(df['ganancia-peso(s-n)'])

px.histogram(df['barro-espinilla(s-n)'])

px.histogram(df['comida-rapida(s-n)'])

px.histogram(df['ejercicio-regular(s-n)'])

"""Análisis gráfico variables no categóricas"""

px.box(df['edad-a'], points='all')

px.box(df['peso-kg'], points='all')

px.box(df['estatura-cm'], points='all')

px.box(df['imc'], points='all')

px.box(df['grupo-sanguineo'], points='all')

px.box(df['frecuencia-cardiaca-bpm'], points='all')

px.box(df['frecuencia-respiratoria-respiraciones/min'], points='all')

px.box(df['hemoglobina-g/dl'], points='all')

px.box(df['ciclo-r/i'], points='all')

px.box(df['duracion-ciclo-d'], points='all')

px.box(df['tiempo-casada-a'], points='all')

px.box(df['nro-abortos'], points='all')

px.histogram(df['nro-abortos'])

px.box(df['h-beta-hcg-I-mIU/mL'], points='all')

px.histogram(df['h-beta-hcg-I-mIU/mL'])

px.box(df['h-beta-hcg-II-mIU/mL'], points='all')

px.histogram(df['h-beta-hcg-II-mIU/mL'])

px.box(df['h-fsh-mIU/mL'], points='all')

px.histogram(df['h-fsh-mIU/mL'])

px.box(df['h-lh-mIU/mL'], points='all')

px.box(df['h-fsh/h-lh'], points='all')

px.box(df['cadera-pulg'], points='all')

px.box(df['cintura-pulg'], points='all')

px.box(df['ind-cintura/cadera'], points='all')

px.box(df['h-tsh-mIU/L'], points='all')

px.box(df['h-amh-ng/mL'], points='all')

px.box(df['h-prl-ng/mL'], points='all')

px.box(df['ex-vit-d3-ng/mL'], points='all')

px.box(df['h-prg-ng/mL'], points='all')

px.box(df['ex-rbs-mg/dl'], points='all')

px.box(df['ps-sistolica-mmHg'], points='all')

px.box(df['ps-diastolica-mmHg'], points='all')

px.box(df['nro-foliculos-ovario-izq'], points='all')

px.box(df['nro-foliculos-ovario-der'], points='all')

px.box(df['prom-tam-foliculos-ovario-izq-mm'], points='all')

px.box(df['prom-tam-foliculos-ovario-der-mm'], points='all')

px.box(df['endometrio-mm'], points='all')

#Histograma de variable determinada para remover outliers
figura1 = px.histogram(df, x='ps-diastolica-mmHg', nbins=30)
figura1.show()

#Histograma de variable determinada para remover outliers
figura2 = px.histogram(df, x='ps-sistolica-mmHg', nbins=30)
figura2.show()

#Histograma de variable determinada para remover outliers
figura3 = px.histogram(df, x='endometrio-mm', nbins=30)
figura3.show()

#Histograma de variable determinada para remover outliers
figura4 = px.histogram(df, x='prom-tam-foliculos-ovario-izq-mm', nbins=60)
figura4.show()

#Histograma de variable determinada para remover outliers
figura5 = px.histogram(df, x='prom-tam-foliculos-ovario-der-mm', nbins=60)
figura5.show()

#Histograma de variable determinada para remover outliers
figura6 = px.histogram(df, x='frecuencia-cardiaca-bpm', nbins=30)
figura6.show()

# Teniendo en cuenta el concepto medico procedemos a identificar y sacar datos outliers
# Aquí discrepamos y preferimos mantener ciertos valores que resultan mas reales y significativos
# a largo plazo en algunos outliers propuestos por la autora original del estudio.

df = df[(df['ps-diastolica-mmHg']>20)]
df = df[(df['ps-sistolica-mmHg']>20)]
df = df[(df['endometrio-mm']>0)]
df = df[(df['prom-tam-foliculos-ovario-izq-mm']>0)]
df = df[(df['prom-tam-foliculos-ovario-der-mm']>0)]
df = df[(df['frecuencia-cardiaca-bpm']>20)]
df = df[(df['ciclo-r/i']<4.5)]

df.shape

"""Los motivos de exclusion para estos registros fueron tomados teniendo en cuenta el concepto de los expecialistas en el área específica de conocimiento. Según concepto de ellos:

- las presiones diastólicas, sistólicas y la frecuencia cardiaca inferiores en valor a 20 con incompatibles con la vida.

- Un endometrio con un tamaño menor a cero es un error de digitación por parte del personal encargado de generear estos informes inicialmente, teniendo en cuenta que estos alimentaron el dataframe para el estudio.

- El tamaño de los folículos en ambos ovarios si y solo sí, debe ser un valor positivo; en cualquier escenario contrario esto significa en error de llenado por parte del personal humano a la hora de rendir el informe con el cual se llenó el dataframe.

- En cuestion a los ciclos se determinó que era un varialble dicotómica por ende cualquier respues que estuviera fuera de su rango es decir un valor mayor a 4, significa o da a entender nuevamente un error de digitación por ende esos mismos son excluidos.

Estos criterios de exclusión fueron tenidos en cuenta porteriormentes a una reunión con especialistas en el dominio del tema.

Como se muestra a continuación ya el valor mínimo es superior a cero en todos los parámetros excepto los que originalmente eran categóricos con respuesta (s-n), para los cuales no aplica el criterio.
"""

df.describe()

px.box(df['ps-diastolica-mmHg'], points='all')

#Histograma de variable sin el outlier
figura7 = px.histogram(df, x='ps-diastolica-mmHg', nbins=30)
figura7.show()

px.box(df['ps-sistolica-mmHg'], points='all')

#Histograma de variable sin outliers
figura8 = px.histogram(df, x='ps-sistolica-mmHg', nbins=30)
figura8.show()

#Histograma de variable sin outliers
figura9 = px.histogram(df, x='endometrio-mm', nbins=30)
figura9.show()

#Histograma de variable sin outliers
figura10 = px.histogram(df, x='prom-tam-foliculos-ovario-izq-mm', nbins=60)
figura10.show()

#Histograma de variable sin outliers
figura11 = px.histogram(df, x='prom-tam-foliculos-ovario-der-mm', nbins=60)
figura11.show()

#Histograma de variable sin outliers
figura12 = px.histogram(df, x='frecuencia-cardiaca-bpm', nbins=30)
figura12.show()

# Grafico de caja sin outlier para ciclo regular o irregular
px.box(df['ciclo-r/i'], points='all')

"""**Visualizacion de datos**"""

#Relacion entre las variables
sns.pairplot(df)

#Determinamos una matriz de correlación de todos los valores de cada parametro

# sacar las variables categoricas del grafico de correlacion.
corrmat = df.corr()
plt.subplots(figsize=(18,18))
sns.heatmap(corrmat,cmap="Pastel1", square=True);

"""**Analisis univariado y multivariado**"""

# Determinamos el valor de correlacion de todos los parametros para el parametro pcos(s-n)
# desde el más significativo al menos
corrmat = df.corr()
corrmat['pcos(s-n)'].sort_values(ascending=False)

#Bastantes variables tienen correlación significativa

plt.figure(figsize=(18,18))
k = 12 #número de variables con mapa de calor positivo
l = 3 #número de variables con mapa de calor negativo
cols_p = corrmat.nlargest(k, "pcos(s-n)")["pcos(s-n)"].index
cols_n = corrmat.nsmallest(l, "pcos(s-n)")["pcos(s-n)"].index
cols = cols_p.append(cols_n)

cm = np.corrcoef(df[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True,cmap="Pastel1", annot=True, square=True, fmt='.2f',
                 annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

#Duración de la fase menstrual en sindrome de ovario poliquistivo vs normal
color = ["teal", "plum"]
figura13 = sns.lmplot(data=df,x="edad-a",y="duracion-ciclo-d", hue="pcos(s-n)",palette=color)
plt.show(figura13)

# Patrón de aumento de peso (IMC) a lo largo de los años en pacientes con pcos y normal.
figura14= sns.lmplot(data =df,x="edad-a",y="imc", hue="pcos(s-n)", palette= color )
plt.show(figura14)

# Patron del ciclo menstrual (r/i) con la edad y el pcos
sns.lmplot(data =df,x="edad-a",y="ciclo-r/i", hue="pcos(s-n)",palette=color)
plt.show()

"""Aunque no es una convencíon después del análisis de evidencia que probablemente el tipo de respuesta igual a 2 equivale a un ciclo irregular y 4 a un ciclo regular.

ciclo-r/i = 2 para ciclos irregulares
ciclo-r/i = 4 para ciclos regulares

"""

# Distribución de folicutlos en ambos ovarios
sns.lmplot(data =df,x='nro-foliculos-ovario-izq',y='nro-foliculos-ovario-der',
           hue='pcos(s-n)',palette=color)
plt.show()

caracteristicas_fol = ["nro-foliculos-ovario-izq","nro-foliculos-ovario-der"]
for i in caracteristicas_fol:
    sns.swarmplot(x=df["pcos(s-n)"], y=df[i], color="black", alpha=0.5 )
    sns.boxenplot(x=df["pcos(s-n)"], y=df[i], palette=color)
    plt.show()

caracteristicas = ['edad-a', 'peso-kg', 'imc', 'hemoglobina-g/dl', 'duracion-ciclo-d', 'endometrio-mm']
for i in caracteristicas:
    sns.swarmplot(x=df['pcos(s-n)'], y=df[i], color="black", alpha=0.5 )
    sns.boxenplot(x=df['pcos(s-n)'], y=df[i], palette=color)
    plt.show()

"""**Conclusiones**

La duración de la fase menstrual es, en general, constante en diferentes edades en los casos normales. Mientras que en el caso del PCOS la duración aumentó con la edad.

El índice de masa corporal (IMC) muestra consistencia en los casos normales. Mientras que en el caso del síndrome de ovario poliquístico, el IMC aumenta con la edad.

El ciclo menstrual se vuelve más regular en los casos normales con la edad. Mientras que en el caso del síndrome de ovario poliquístico la irregularidad aumenta con la edad.

La distribución de los folículos en ambos ovarios, izquierdo y derecho, no es igual para las mujeres con síndrome de ovario poliquístico en comparación con la paciente "normal".

La cantidad de folículos en mujeres con pcos es mayor, como se esperaba. Y también son desiguales.

La muestra es corta para determinar si hay hechos patognómonicas que arrojen nueva información a los estudios y conociemientos actuales. Se recomienda aumentar la muestra.

###4. Modelado

<html lang="es">

<body>
    <h2>Evaluación de Modelos de Clasificación para Predicción del Síndrome de Ovarios Poliquísticos</h2>
    <p>
        Para abordar el objetivo de este ejercicio, que consiste en desarrollar un modelo capaz de predecir si una mujer es propensa a padecer síndrome de ovarios poliquísticos, se evaluarán inicialmente un total de ocho modelos de clasificación. Estos modelos son:
    </p>
    <ul>
        <li>Regresión Logística</li>
        <li>KNN</li>
        <li>Naive Bayes</li>
        <li>Árboles de Decisión con criterio ID3</li>
        <li>Árboles de Decisión con criterio CART</li>
        <li>Bagging</li>
        <li>Random Forest</li>
        <li>XGBoost</li>
    </ul>
    <h2>Escalado de los Datos</h2>
    <p>
            Dadas las diferentes escalas de las variables, se opta por realizar una estandarización de los datos. Este proceso escrucial para garantizar que todas las variables contribuyan de manera equitativa al modelo, evitando que aquellas con mayor magnitud dominen el proceso de aprendizaje. El escalado de datos mejora la eficiencia y el rendimiento del modelo, asegurando una mejor convergencia durante el entrenamiento y permitiendo una comparación más justa entre las diferentes variables.
    </p>
    <h2>Métricas Clave</h2>
    <p>
        <strong>Accuracy:</strong> Esta métrica permite determinar la precisión del modelo en términos de aciertos tanto en mujeres que tienen como en las que no tienen el síndrome de ovarios poliquísticos.
    </p>
    <p>
        <strong>Recall:</strong> Dado que es crucial minimizar la cantidad de falsos negativos (mujeres a las que se les predice que no tienen el síndrome cuando en realidad sí lo tienen), el recall es esencial para evaluar la capacidad del modelo para identificar correctamente los casos positivos.
    </p>
    <h2>Protocolos de Evaluación</h2>
    <p>
        Para obtener una evaluación más completa y llegar al modelo óptimo, cada uno de los modelos se someterá a dos fases de análisis:
    </p>
    <ol>
        <li><strong>Fase sin búsqueda de hiperparámetros:</strong> En esta fase inicial, los modelos se evaluarán con sus configuraciones por defecto.</li>
        <li><strong>Fase con búsqueda de hiperparámetros:</strong> En la segunda fase, se realizará una optimización de hiperparámetros utilizando los protocolos de validación cruzada K-fold y GridSearchCV.</li>
    </ol>
    <h2>Esquema del Proceso</h2>
    <p>El esquema gráfico del proceso es el siguiente:</p>
    <img src="https://raw.githubusercontent.com/leoe21/images/main/image.png" alt="Esquema de evaluación de modelos" style="width:100%;max-width:600px;">
    <h2>Selección del Modelo</h2>
    <ol>
        <li><strong>Selección del modelo:</strong> Evaluar todos los modelos listados y calcular las métricas respectivas (accuracy y recall).</li>
        <li><strong>Optimización del modelo seleccionado:</strong> Seleccionar el modelo con mejor rendimiento y realizar una búsqueda de hiperparámetros más exhaustiva utilizando GridSearchCV y K-fold cross validation para obtener el modelo definitivo.</li>
    </ol>
    <p>
        Este enfoque nos permitirá identificar el modelo con mejor desempeño en la predicción del síndrome de ovarios poliquísticos, asegurando un equilibrio entre precisión y capacidad para detectar casos positivos.
    </p>
</body>
</html>
"""

# Codificando en 1 y 0 ciclo regular
df["ciclo-r/i"]=df["ciclo-r/i"].replace({4:1,2:0})
df.rename(columns={"ciclo-r/i":"ciclo-r"}, inplace=True)

"""<strong>Modelo 1: Regresión Logística</strong>
<br><br>
Presentamos versiones de cada modelo, uno sin busqueda de hiérparametros y otro con busqueda.

<strong>Regresión Logística sin busqueda de hiperperparametros óptimos</strong>
<br><br>
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, recall_score
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score
import seaborn as sns


###Modelo de regresión Logisitica


# Preparar los datos
# Separar las características (X) de la variable objetivo (y)
X = df.drop(columns=['pcos(s-n)'])
y = df['pcos(s-n)']

# Escalar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Paso 3: Dividir el dataset en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Entrenar el modelo de regresión logística con un mayor número de iteraciones
model = LogisticRegression(max_iter=5000, solver='lbfgs')
model.fit(X_train, y_train)

# Evaluar el modelo
y_pred_rl = model.predict(X_test)

# Calcular la precisión
accuracy = accuracy_score(y_test, y_pred_rl)
print(f'Accuracy: {accuracy:.2f}')

# Matriz de confusión
conf_matrix_rl = confusion_matrix(y_test, y_pred_rl)
print('Matriz de confusión:')
print(conf_matrix_rl)

# Reporte de clasificación
class_report_rl = classification_report(y_test, y_pred_rl)
print('Reporte:')
print(class_report_rl)

recall_class_rl = recall_score(y_test, y_pred_rl, pos_label=0)

# Crear Matriz
sns.heatmap(conf_matrix_rl, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<br><br>
<strong>Regresión Logística con busqueda de hiperperparametros óptimos</strong>
"""

# Definir los hiperparámetros a ajustar
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l2', 'none']
}

# Definir el protocolo de evaluación K-fold
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Inicializar el GridSearchCV
grid_search = GridSearchCV(estimator=LogisticRegression(max_iter=5000, solver='lbfgs'),
                           param_grid=param_grid,
                           cv=kfold,
                           scoring='accuracy',
                           verbose=1,
                           n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Mejor modelo
best_model = grid_search.best_estimator_


cv_scores = cross_val_score(best_model, X_test, y_test, cv=kfold)
print("Cross Validation Scores:", cv_scores)

# Calcular la precisión media de la validación cruzada
mean_accuracy = cv_scores.mean()
print(f'Mean Accuracy (Cross Validation): {mean_accuracy:.2f}')


# Evaluar utilizando recall
cv_scores_recall = cross_val_score(best_model, X_test, y_test, cv=kfold, scoring='recall')
print("Cross Validation Recall Scores:", cv_scores_recall)

# Calcular el recall medio de la validación cruzada
mean_recall = cv_scores_recall.mean()
print(f'Mean Recall (Cross Validation): {mean_recall:.2f}')

#Matriz de confusión
y_pred_cv = cross_val_predict(best_model, X_test, y_test, cv=kfold)

# Calcular la matriz de confusión
conf_matrix = confusion_matrix(y_test, y_pred_cv)
print('Confusion Matrix:')
print(conf_matrix)


# Crear una figura y un eje (axis) para la visualización
plt.figure(figsize=(8, 6))
sns.set(font_scale=1.4)  # Ajustar el tamaño de fuente


class_report_rl_2 = classification_report(y_test, y_pred_cv)
print('Reporte:')
print(class_report_rl_2)

# Crear el heatmap
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""### Conclusión:

Al corres el modelo de regresión logística y el modelo con búsqueda de hiperparámetros se puede evidenciar que el segundo modelo es arroja un mejor resultado más preciso pasando de un 85% a un 88%, indicando en cuanto a las predicciones es mucho mas acertado.

<strong>Modelo: K vecinos más cercanos</strong>

<strong>K vecinos más cercanos sin busqueda de hiperparametros</strong>
"""

from sklearn.neighbors import KNeighborsClassifier

#Modelo KNN

k = 5
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)

# Evaluar el modelo
y_pred_knn = knn.predict(X_test)

# Calcular la precisión
accuracy_knn = accuracy_score(y_test, y_pred_knn)
print(f'Accuracy: {accuracy_knn:.2f}')

# Matriz de confusión
conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)
print('Matriz de confusión:')
print(conf_matrix_knn)

# Reporte de clasificación
class_report_knn = classification_report(y_test, y_pred_knn)
print('Reporte:')
print(class_report_knn)

# Crear Matriz
sns.heatmap(conf_matrix_knn, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>K vecinos más cercanos con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros a ajustar
param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Número de vecinos
    'weights': ['uniform', 'distance'],  # Test pesos uniformes
    'metric': ['euclidean', 'manhattan']  # Test distancias
}

# Definir el protocolo de evaluación K-fold
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Inicializar el GridSearchCV
grid_search = GridSearchCV(estimator=KNeighborsClassifier(),
                           param_grid=param_grid,
                           cv=kfold,
                           scoring='accuracy',
                           verbose=1,
                           n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_knn = grid_search.best_params_
print("Best Parameters:", best_params_knn)

# Mejor modelo
best_model = grid_search.best_estimator_


cv_scores_knn = cross_val_score(best_model, X_train, y_train, cv=kfold)
print("Cross Validation Scores:", cv_scores_knn)

# Calcular la precisión media de la validación cruzada
mean_accuracy = cv_scores.mean()
print(f'Mean Accuracy (Cross Validation): {mean_accuracy:.2f}')

#Matriz de confusión
y_pred_cv_knn = cross_val_predict(best_model, X_test, y_test, cv=kfold)

# Calcular la matriz de confusión
conf_matrix_knn = confusion_matrix(y_test, y_pred_cv_knn)
print('Confusion Matrix:')
print(conf_matrix_knn)


# Crear una figura y un eje (axis) para la visualización
plt.figure(figsize=(8, 6))
sns.set(font_scale=1.4)  # Ajustar el tamaño de fuente


class_report_knn_2 = classification_report(y_test, y_pred_cv_knn)
print('Reporte:')
print(class_report_knn_2)

# Crear el heatmap
sns.heatmap(conf_matrix_knn, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""###Conclusión:
Al corres el modelo de KNN y el modelo con búsqueda de hiperparámetros se puede evidenciar que el segundo modelo es arroja un mejor resultado más preciso pasando de un 84% a un 88%, indicando en cuanto a las predicciones es mucho mas acertado.

<strong>Modelo 3: Naive Bayes</strong>
"""

from sklearn.naive_bayes import GaussianNB
# Modelo Naive Bayes

# Entrenar el modelo Naive Bayes

nb = GaussianNB()
nb.fit(X_train, y_train)

# Evaluar el modelo
y_pred_nb = nb.predict(X_test)

# Calcular la precisión
accuracy_nb = accuracy_score(y_test, y_pred_nb)
print(f'Accuracy: {accuracy_nb:.2f}')

# Matriz de confusión
conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)
print('Confusion Matrix:')
print(conf_matrix)

# Reporte de clasificación
class_report_NB = classification_report(y_test, y_pred_nb)
print('Classification Report:')
print(class_report_NB)

# Crear Matriz
sns.heatmap(conf_matrix_nb, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión')

# Mostrar la visualización
plt.show()

"""###Conclusión:

Al ejecutar el modelo nos arroja una exactitud del 78% y un recall 75% Indicando que el modelo puede tener cierta dificultad en identificar todas las instancias positivas correctamente.

<strong>Modelo 4: Arboles de decisión con criterio ID3</strong>

<strong>Arboles de decisión con criterio ID3 sin busqueda de hiperparametros</strong>
"""

from sklearn.tree import DecisionTreeClassifier

### Arboles de desicion con criteriod ID3 (Entropia)

# Entrenar el modelo de árbol de decisión usando el criterio "entropy" (ID3)
dt = DecisionTreeClassifier(criterion='entropy', random_state=42)
dt.fit(X_train, y_train)

# Evaluar el modelo
y_pred_id3 = dt.predict(X_test)

# Calcular la precisión
accuracy_id3 = accuracy_score(y_test, y_pred_id3)
print(f'Accuracy: {accuracy_id3:.2f}')

# Matriz de confusión
conf_matrix_id3 = confusion_matrix(y_test, y_pred_id3)
print('Matriz de confusión:')
print(conf_matrix_id3)

# Reporte de clasificación
class_report_ID3 = classification_report(y_test, y_pred_id3)
print('Classification Report:')
print(class_report_ID3)

# Crear Matriz
sns.heatmap(conf_matrix_id3, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Arboles de decisión con criterio ID3 con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros a ajustar
param_grid = {
    'max_depth': [None, 5, 10, 15],  # Vary the maximum depth of the tree
    'min_samples_split': [2, 5, 10],  # Vary the minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]  # Vary the minimum number of samples required to be at a leaf node
}

# Definir el protocolo de evaluación K-fold
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Inicializar el GridSearchCV
grid_search_id3 = GridSearchCV(estimator=DecisionTreeClassifier(criterion='entropy', random_state=42),
                           param_grid=param_grid,
                           cv=kfold,
                           scoring='accuracy',
                           verbose=1,
                           n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_id3.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_id3 = grid_search.best_params_
print("Best Parameters:", best_params_id3)

# Mejor modelo
best_model_id3 = grid_search_id3.best_estimator_

y_pred_id3_2 = best_model_id3.predict(X_test)

# Calcular la precisión
accuracy_id3_2 = accuracy_score(y_test, y_pred_id3_2)
print(f'Accuracy: {accuracy_id3_2:.2f}')

# Matriz de confusión
conf_matrix_id3_2 = confusion_matrix(y_test, y_pred_id3_2)
print('Matriz de confusión:')
print(conf_matrix_id3_2)

# Reporte de clasificación
class_report_ID3_2 = classification_report(y_test, y_pred_id3_2)
print('Classification Report:')
print(class_report_ID3_2)

# Crear Matriz
sns.heatmap(conf_matrix_id3_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""###Conclusión:
En cuanto a los modelos de arboles de decisión con ID3 de entropía y el modelo con búsqueda de hiperparámetros los dos modelos arrojaron el mismo resultado de accuracy 79%, y tiene un recall de 83% y 87% respectivamente, nos indica que ambos modelos lograron un accuracy comparable, el modelo con búsqueda de hiperparámetros demostró una mejor capacidad para detectar los casos positivos

<strong>Modelo 5: Arboles de decisión con criterio CART</strong>

<strong>Arboles de decisión con criterio CART sin busqueda de hiperparametros</strong>
"""

# Entrenar el modelo de árbol de decisión usando el criterio "gini" (CART)
dt_cart = DecisionTreeClassifier(criterion='gini', random_state=42)
dt_cart.fit(X_train, y_train)

# Evaluar el modelo
y_pred_cart = dt_cart.predict(X_test)

# Calcular la precisión
accuracy_cart = accuracy_score(y_test, y_pred_cart)
print(f'Accuracy: {accuracy_cart:.2f}')

# Matriz de confusión
conf_matrix_cart = confusion_matrix(y_test, y_pred_cart)
print('Matriz de confusión:')
print(conf_matrix_cart)

# Reporte de clasificación
class_report_CART = classification_report(y_test, y_pred_cart)
print('Classification Report:')
print(class_report_CART)

# Crear Matriz
sns.heatmap(conf_matrix_cart, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Arboles de decisión con criterio CART con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros a ajustar
param_grid = {
    'max_depth': [None, 5, 10, 15],  # Vary the maximum depth of the tree
    'min_samples_split': [2, 5, 10],  # Vary the minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]  # Vary the minimum number of samples required to be at a leaf node
}

# Definir el protocolo de evaluación K-fold
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Inicializar el GridSearchCV
grid_search_cart = GridSearchCV(estimator=DecisionTreeClassifier(criterion='gini', random_state=42),
                           param_grid=param_grid,
                           cv=kfold,
                           scoring='accuracy',
                           verbose=1,
                           n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_cart.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_cart = grid_search_cart.best_params_
print("Best Parameters:", best_params_cart)

# Mejor modelo
best_model_cart = grid_search_cart.best_estimator_

y_pred_cart_2 = best_model_cart.predict(X_test)


# Calcular la precisión
accuracy_cart_2 = accuracy_score(y_test, y_pred_cart_2)
print(f'Accuracy: {accuracy_cart_2:.2f}')

# Matriz de confusión
conf_matrix_cart_2 = confusion_matrix(y_test, y_pred_cart_2)
print('Matriz de confusión:')
print(conf_matrix_cart_2)

# Reporte de clasificación
class_report_cart_2 = classification_report(y_test, y_pred_cart_2)
print('Classification Report:')
print(class_report_cart_2)

# Crear Matriz
sns.heatmap(conf_matrix_cart_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""###Conclusión:

El modelo de arboles de decisión con CART de entropía y el modelo con búsqueda de hiperparámetros, estós arrojaron como resultado de accuracy 82% y 83%, y tiene un recall de 86% y 81% respectivamente mostrando que el segundo modelo obtuvo una precisión ligeramente superior, el primer modelo demostró ser más robusto en términos de recall, lo que puede ser crucial en situaciones donde la identificación precisa de instancias positivas es de alta.

<strong>Modelo 6: Bagging</strong>

<strong>Bagging sin busqueda de hiperparametros</strong>
"""

from sklearn.ensemble import BaggingClassifier

# Modelo Baggin (usando como base estimador de gini)
# Entrenar el modelo Bagging con un árbol de decisión
base_estimator = DecisionTreeClassifier(criterion='gini', random_state=42)
bagging = BaggingClassifier(estimator=base_estimator, n_estimators=100, random_state=42)
bagging.fit(X_train, y_train)

# Evaluar el modelo
y_pred_bg = bagging.predict(X_test)

# Calcular la precisión
accuracy_bg = accuracy_score(y_test, y_pred_bg)
print(f'Accuracy: {accuracy_bg:.2f}')

# Matriz de confusión
conf_matrix_bg = confusion_matrix(y_test, y_pred_bg)
print('Confusion Matrix:')
print(conf_matrix_bg)

# Reporte de clasificación
class_report_Bag = classification_report(y_test, y_pred_bg)
print('Classification Report:')
print(class_report_Bag)

# Crear Matriz
sns.heatmap(conf_matrix_bg, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Bagging con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros para el árbol de decisión
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Definir el protocolo de evaluación K-fold
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Inicializar GridSearchCV para el árbol de decisión
grid_search_dt = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),
                              param_grid=param_grid,
                              cv=kfold,
                              scoring='accuracy',
                              verbose=1,
                              n_jobs=-1)

# Entrenar el árbol de decisión utilizando GridSearchCV
grid_search_dt.fit(X_train, y_train)

# Mejores hiperparámetros encontrados para el árbol de decisión
best_params_dt = grid_search_dt.best_params_
print("Best Parameters for Decision Tree:", best_params_dt)

# Construir el modelo de Bagging utilizando el árbol de decisión con los hiperparámetros optimizados
base_estimator_dt = DecisionTreeClassifier(random_state=42, **best_params_dt)
bagging_2 = BaggingClassifier(estimator=base_estimator_dt, n_estimators=100, random_state=42)

# Entrenar el modelo de Bagging
bagging_2.fit(X_train, y_train)

# Evaluar el modelo de Bagging
y_pred_bg_2 = bagging_2.predict(X_test)

# Calcular la precisión
accuracy_bg_2 = accuracy_score(y_test, y_pred_bg_2)
print(f'Accuracy: {accuracy_bg_2:.2f}')

# Matriz de confusión
conf_matrix_bg_2 = confusion_matrix(y_test, y_pred_bg_2)
print('Confusion Matrix:')
print(conf_matrix_bg_2)

# Reporte de clasificación
class_report_Bag_2 = classification_report(y_test,  y_pred_bg_2)
print('Classification Report:')
print(class_report_Bag_2)

# Matriz de confusión
conf_matrix_bg_2 = confusion_matrix(y_test, y_pred_bg_2)
print('Confusion Matrix:')
print(conf_matrix_bg_2)

# Crear Matriz
sns.heatmap(conf_matrix_bg_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""###Conclusión:

Se corre el modelo de Bagging y el modelo con búsqueda de hiperparámetros arrojaron como resultado de accuracy 87%, y tiene un recall de 91% y 92% respectivamente, evidenciando que el segundo modelo demostró una capacidad ligeramente superior para identificar correctamente las instancias positivas en comparación con el primero.

<strong>Modelo 7: Random Forest</strong>

<strong> Random Forest sin busqueda de hiperparametros</strong>
"""

from sklearn.ensemble import RandomForestClassifier

#Modelo Random Forest

# Entrenar el modelo Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Evaluar el modelo
y_pred_rf = rf.predict(X_test)

# Calcular la precisión
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f'Accuracy: {accuracy_rf:.2f}')

# Matriz de confusión
conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
print('Confusion Matrix:')
print(conf_matrix_rf)

# Reporte de clasificación
class_report_RF = classification_report(y_test, y_pred_rf)
print('Classification Report:')
print(class_report_RF)

# Crear Matriz
sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong> Random Forest con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros a ajustar
param_grid = {
'n_estimators': [50, 100, 150],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Definir el protocolo de evaluación K-fold
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Inicializar GridSearchCV para el Random Forest
grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                              param_grid=param_grid,
                              cv=kfold,
                              scoring='accuracy',
                              verbose=1,
                              n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_rf.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_rf = grid_search_rf.best_params_
print("Best Parameters for Random Forest:", best_params_rf)

# Mejor modelo
best_model_rf = grid_search_rf.best_estimator_

# Evaluar el modelo en los datos de prueba
y_pred_rf_2 = best_model_rf.predict(X_test)

# Calcular la precisión
accuracy_rf_2 = accuracy_score(y_test, y_pred_rf_2)
print(f'Accuracy: {accuracy_rf_2:.2f}')

# Matriz de confusión
conf_matrix_rf_2 = confusion_matrix(y_test, y_pred_rf_2)
print('Confusion Matrix:')
print(conf_matrix_rf_2)

# Reporte de clasificación
class_report_RF_2 = classification_report(y_test, y_pred_rf_2)
print('Classification Report:')
print(class_report_RF_2)

# Crear Matriz
sns.heatmap(conf_matrix_rf_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""###Conclusión:

En los modelos de Random Forest y el modelo con búsqueda de hiperparámetros los modelos arrojaron como resultado de accuracy 89% y 91%, y tiene un recall de 94% y 96% respectivamente, evidenciando que el segundo modelo demostró una capacidad superior para identificar correctamente las instancias positivas en comparación con el primero.

<strong>Modelo 8: XGBoost</strong>

# <strong>XGBoost sin busqueda de hiperparametros</strong>
"""

import xgboost as xgb

# XGBoost

# Entrenar el modelo XGBoost
xgb_model = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
xgb_model.fit(X_train, y_train)

# Evaluar el modelo
y_pred_xg = xgb_model.predict(X_test)

# Calcular la precisión
accuracy_xg = accuracy_score(y_test, y_pred_xg)
print(f'Accuracy: {accuracy_xg:.2f}')

# Matriz de confusión
conf_matrix_xg = confusion_matrix(y_test, y_pred_xg)
print('Confusion Matrix:')
print(conf_matrix_xg)

# Reporte de clasificación
class_report_XGB = classification_report(y_test, y_pred_xg)
print('Classification Report:')
print(class_report_XGB)

# Crear Matriz
sns.heatmap(conf_matrix_xg, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>XGBoost con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros a ajustar
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0],
    'max_depth': [3, 5, 7, 9, 11],
    'n_estimators': [50, 100, 150, 200, 250],
}

# Definir el protocolo de evaluación K-fold
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Inicializar GridSearchCV para XGBoost
grid_search_xgb = GridSearchCV(estimator=xgb.XGBClassifier(objective="binary:logistic", random_state=42),
                                param_grid=param_grid,
                                cv=kfold,
                                scoring='accuracy',
                                verbose=1,
                                n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_xgb.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_xgb = grid_search_xgb.best_params_
print("Best Parameters for XGBoost:", best_params_xgb)

# Mejor modelo
best_model_xgb = grid_search_xgb.best_estimator_

# Evaluar el modelo en los datos de prueba
y_pred_xgb = best_model_xgb.predict(X_test)

# Calcular la precisión
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f'Accuracy: {accuracy_xgb:.2f}')

# Matriz de confusión
conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)
print('Confusion Matrix:')
print(conf_matrix_xgb)

# Reporte de clasificación
class_report_XGB = classification_report(y_test, y_pred_xgb)
print('Classification Report:')
print(class_report_XGB)

# Crear Matriz
sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""###Conclusión:

Al correr el modelo de modelos de XGBoost y el modelo con búsqueda de hiperparámetros los modelos arrojaron como resultado de accuracy 86% y 85%, y tiene un recall de 91% y 91% respectivamente, En este caso específico, la búsqueda de hiperparámetros no genero un impacto importante en el rendimiento del modelo en términos de recall, aunque hubo una ligera diferencia en el accuracy. Esto indica que los hiperparámetros predeterminados en el modelo sin búsqueda de hiperparámetros fueron lo suficientemente buenos como para lograr un rendimiento comparable al modelo optimizado

###**RESUMEN DE RESULTADOS DE LOS MODELOS USADOS**
"""

# Lista de nombres de las predicciones sin búsqueda y con búsqueda
predicciones = {
    "Sin busqueda": [y_pred_rl, y_pred_knn, y_pred_nb, y_pred_id3, y_pred_cart, y_pred_bg, y_pred_rf, y_pred_xg],
    "Con busqueda": [y_pred_cv, y_pred_cv_knn, None, y_pred_id3_2, y_pred_cart_2, y_pred_bg_2, y_pred_rf_2, y_pred_xgb]
}

nombres_modelos = [
    "Regresion logistica", "KNN", "Bayes", "Arbol de desicion ID3",
    "Arbol de desicion Cart", "Bagging", "Random forest", "XGBoost"
]

# Inicializar diccionario para almacenar los resultados
resultados = {
    "Modelo": nombres_modelos,
    "Accuracy (Sin busqueda)": [],
    "Recall (Sin busqueda)": [],
    "Accuracy (Con busqueda)": [],
    "Recall (Con busqueda)": []
}

# Calcular las métricas
for i, modelo in enumerate(nombres_modelos):
    # Sin busqueda
    y_pred_sin = predicciones["Sin busqueda"][i]
    if y_pred_sin is not None:
        resultados["Accuracy (Sin busqueda)"].append(accuracy_score(y_test, y_pred_sin))
        resultados["Recall (Sin busqueda)"].append(recall_score(y_test, y_pred_sin, pos_label=0))
    else:
        resultados["Accuracy (Sin busqueda)"].append(None)
        resultados["Recall (Sin busqueda)"].append(None)

    # Con busqueda
    y_pred_con = predicciones["Con busqueda"][i]
    if y_pred_con is not None:
        resultados["Accuracy (Con busqueda)"].append(accuracy_score(y_test, y_pred_con))
        resultados["Recall (Con busqueda)"].append(recall_score(y_test, y_pred_con, pos_label=0))
    else:
        resultados["Accuracy (Con busqueda)"].append(None)
        resultados["Recall (Con busqueda)"].append(None)

# Crear un DataFrame con los resultados
df_resultados = pd.DataFrame(resultados)


df_resultados

"""<body>
    <h1>Selección de Modelo para la Predicción del Síndrome de Ovario Poliquístico</h1>
    <p>Después de evaluar varios modelos de aprendizaje automático, hemos determinado que el modelo de <strong>Random Forest con búsqueda de hiperparámetros</strong> es el más adecuado para predecir el síndrome de ovario poliquístico. Este modelo ha mostrado un rendimiento superior en comparación con los demás modelos evaluados, alcanzando una <strong>precisión (accuracy) de 0.91</strong> y una <strong>recuperación (recall) de 0.96</strong>. Estas métricas indican que el modelo no solo es capaz de predecir correctamente una alta proporción de los casos, sino que también tiene una excelente capacidad para identificar todos los casos positivos de la enfermedad.</p>
    <p>En resumen, la combinación de alta precisión y alta recuperación obtenida mediante la optimización de hiperparámetros hace que el modelo de Random Forest sea nuestra mejor opción para esta tarea. Este rendimiento robusto sugiere que el modelo es confiable y eficiente para el diagnóstico del síndrome de ovario poliquístico, lo que puede contribuir significativamente a la mejora en la identificación y el tratamiento oportuno de esta condición.</p>
</body>
"""

print("Los mejores hiperparametros para el modelo seleccionado fueron los siguientes:", best_params_rf)

"""# COMPONENTES DE ANÁLISIS PRINCIPALES (PCA)

El Análisis de Componentes Principales (PCA, por sus siglas en inglés) es una técnica de reducción de dimensionalidad ampliamente utilizada en la ciencia de datos y el análisis de datos. Su objetivo principal es transformar un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas, denominadas componentes principales.

El dataset de PCOS contiene muchas variables. PCA reduce el número de variables al encontrar nuevas variables no correlacionadas (componentes principales) que son combinaciones lineales de las originales y que explican la mayor parte de la varianza en los datos.

### Escalado de Datos

Primero, escalamos los datos utilizando StandardScaler de scikit-learn. Esto asegura que todas las características tengan una media de 0 y una desviación estándar de 1, lo cual es importante para PCA, ya que es sensible a las escalas de las variables.
"""

# Escalar los datos
scaler = StandardScaler(with_mean=True, with_std=True)
scaler.fit(df)

# Imprimir la media de cada característica
print(scaler.mean_)

# Transformar los datos escalados
df_std = scaler.transform(df)

"""Segundo, aplicaremos el Análisis de Componentes Principales (PCA) a los datos estandarizados. Utilizaremos la clase PCA de scikit-learn para ajustar el modelo PCA y transformar los datos."""

from sklearn.decomposition import PCA

# Aplicamos PCA
pca = PCA()
df_proyectado = pca.fit_transform(df_std)

"""Revisamos los componentes principales obtenidos del PCA. Los componentes principales son vectores que describen la dirección en el espacio original de las características en la que se encuentra la mayor varianza de los datos."""

# Obtener los componentes principales
print(pca.components_)

df.columns

print(pca.explained_variance_ratio_)

np.sum(pca.explained_variance_ratio_[0:28])

"""A pesar de tener muchas características (42), los primeros pocos componentes principales pueden capturar una proporción significativa de la varianza. Por ejemplo, con solo los primeros 28 componentes, capturamos alrededor del 88.9% de la varianza."""

# Varianza explicada por cada componente principal
var_exp = pca.explained_variance_ratio_

# Varianza acumulada
cum_var_exp = np.cumsum(var_exp)

# Visualización
plt.figure(figsize=(15, 7))
plt.bar(range(len(var_exp)), var_exp, alpha=0.3333, align='center', label='Varianza explicada por cada PC', color='g')
plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid', label='Varianza explicada acumulada')
plt.ylabel('Porcentaje de varianza explicada')
plt.xlabel('Componentes principales')
plt.legend(loc='best')
plt.title('Varianza explicada por cada componente principal y varianza acumulada')
plt.show()

"""## Conclusiones PCA

* El PCA ha permitido reducir la complejidad del dataset original, compuesto por 42 características, a un número menor de componentes principales que capturan la mayoría de la variabilidad en los datos. Esta reducción facilita la interpretación y el análisis posterior.

* Los primeros cinco componentes principales en conjunto explican aproximadamente el 31.82% de la varianza total.
Para explicar más del 50% de la varianza total, se necesitan aproximadamente los primeros 16 componentes principales.

* La varianza acumulada muestra que poco más de los primeros 20 componentes principales son necesarios para capturar aproximadamente el 80% de la varianza total. Esto indica que, aunque se puede reducir la dimensionalidad del dataset, se necesita un número considerable de componentes para mantener una cantidad significativa de información.

# Clustering
"""

df.describe()

"""## K-MEANS"""

from sklearn.cluster import KMeans

# Filtrar las variables eliminando las dicotómicas
variables_continuas = [var for var in df if '(s-n)' not in var]

# Crear un nuevo DataFrame con solo las variables numéricas continuas
df_continuo = df[variables_continuas]

# Escalar los datos
scaler = StandardScaler()
df_std = scaler.fit_transform(df_continuo)

# Inicializar K-Means
kmeans = KMeans(n_clusters=4, random_state=0)

# Aplicar K-Means
kmeans.fit(df_std)

# Obtener las etiquetas de cluster asignadas a cada muestra
labels = kmeans.labels_

# Obtener las coordenadas de los centroides de los clusters
centroids = kmeans.cluster_centers_

# Crear la gráfica de codo para determinar el número óptimo de clusters
WSSs = []
for i in range(1, 20):
    km = KMeans(n_clusters=i, random_state=0)
    km.fit(df_std)
    WSSs.append(km.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(range(1, 20), WSSs, marker='o')
plt.title('Gráfica de Codo')
plt.xlabel('Número de Clusters')
plt.ylabel('Suma de las distancias cuadráticas dentro del cluster (WSS)')
plt.show()

from sklearn.metrics import silhouette_samples, silhouette_score

k = 2


kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)
kmeans.fit(df_std)
y_clusters = kmeans.labels_
cluster_labels = np.unique(y_clusters)
silhouette_scores = silhouette_samples(df_std, y_clusters, metric='euclidean')

# Configurar la visualización del gráfico de silueta
y_ax_lower, y_ax_upper = 0, 0
yticks = []

# Iterar sobre los clusters
for i, c in enumerate(cluster_labels):
    silhouette_scores_c = silhouette_scores[y_clusters == c]
    silhouette_scores_c.sort()
    y_ax_upper += len(silhouette_scores_c)
    color = plt.cm.tab10(float(i) / k)  # Usar colores predeterminados
    plt.barh(range(y_ax_lower, y_ax_upper), silhouette_scores_c, height=1.0, edgecolor='none', color=color)
    yticks.append((y_ax_lower + y_ax_upper) / 2.)
    y_ax_lower += len(silhouette_scores_c)

# Calcular y graficar la línea de silueta promedio
silhouette_avg = np.mean(silhouette_scores)
plt.axvline(silhouette_avg, color="black", linestyle="--")

plt.yticks(yticks, cluster_labels + 1)
plt.ylabel('Cluster')
plt.xlabel('Coeficiente de silueta')
plt.title('Gráfico de Silueta')
plt.tight_layout()
plt.show()

"""Se exploraron diferentes métricas y técnicas para determinar el número óptimo de clusters, sin embargo, debido a la falta de un codo significativo en el gráfico de codo y la presencia de valores negativos en el coeficiente de silueta, se presenta un desafío en la identificación clara de la estructura de clusters en los datos.

El gráfico de codo se utilizó para identificar el número óptimo de clusters en el algoritmo K-Means. Sin embargo, la representación visual de la suma de las distancias cuadradas intra-cluster (WSS) en función del número de clusters no mostró un punto de inflexión claro o "codo". Esta falta de un punto de codo sugiere que no hay una división clara de los datos en un número específico de clusters.

El coeficiente de silueta se utilizó como una métrica alternativa para evaluar la cohesión y la separación de los clusters. Se observó que algunas muestras tenían valores negativos en el coeficiente de silueta, lo que indica que podrían estar más cerca del centroide de un cluster vecino que del centroide de su propio cluster asignado. Esto pueder ser una posible superposición o ambigüedad en la asignación de clusters.

## WARD (JERÁRQUICO)
"""

from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

Z = linkage(df_std, method='ward')

# Dendrograma
plt.figure(figsize=(12, 6))
dendrogram(Z)
plt.title('Dendrograma')
plt.xlabel('Índices de la muestra')
plt.ylabel('Distancia')
plt.show()

from scipy.cluster.hierarchy import dendrogram, linkage, inconsistent

# Calcular la inconsistencia
depth = 5  # Profundidad de cálculo de la inconsistencia
incons = inconsistent(Z, depth)

# Graficar la inconsistencia
plt.figure(figsize=(12, 6))
plt.plot(range(1, len(incons) + 1), incons[:, 2])
plt.title('Inconsistencia del dendrograma')
plt.xlabel('Número de fusiones')
plt.ylabel('Inconsistencia')
plt.show()

"""Se exploró la estructura de clustering mediante la construcción y análisis de un dendrograma. Basándonos en la observación del dendrograma, se sugiere que un número adecuado de clusters podría ser 5.

Análisis del Dendrograma:
Al observar el dendrograma generado, se puede notar que hay un punto en el cual los clusters se dividen en cuatro grupos distintos. Este punto se caracteriza por una fusión significativa en la altura de las ramas, indicando una potencial división natural del dataset en 5 clusters.

Los puntos del dataset se agrupan en cuatro clusters distintos, cada uno representando una subpoblación potencialmente diferente en el contexto del Síndrome de Ovarios Poliquísticos.

# 7. Bibliografía

- American College of Obstetricians and Gynecologists. (2015). Polycystic ovary syndrome. Obtenido el 20 de mayo de 2016 en http://www.acog.org/Patients/FAQs/Polycystic-Ovary-Syndrome-PCOS en el contenido de Inglés Notificaciόn de salida
- U.S. Department of Health and Human Services, Office on Women’s Health. (2014). Polycystic ovary syndrome (PCOS) fact sheet. Obtenido el 20 de mayo de 2016 en https://espanol.womenshealth.gov/a-z-topics/polycystic-ovary-syndrome
- FONTES, R.; et al. Reference interval of thyroid stimulating hormone and free thyroxine in a reference population over 60 years old and in very old subjects (over 80 years): comparison to young subjects. Thyroid Res. 6. 13, 2013
- WARD, L. S. Devemos mudar os valores de referência para TSH normal?. Arq Bras Endocrinol Metab. 52. 1; 2008

**Diseño asisitido por IA**
"""