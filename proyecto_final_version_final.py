# -*- coding: utf-8 -*-
"""Proyecto_Final_Version_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/alfa7g7/Fundamentos-Analitica-I/blob/main/Proyecto_Final_Version_Final.ipynb

### **Proyecto de Fundamentos Analítica I** *(Daniel Delgado Caicedo, Raúl Alberto Echeverry López, Luis Esteban Ordoñez Erazo, Fabian Salazar Figueroa)*

TABLA DE CONTENIDO

1. Introducción

2. Contexto
    - Pregunta inteligente (SMART)
        - Específico (Specific)
        - Medible (Measurable)
        - Alcanzable (Achievable)
        - Relevante (Relevant)
        - Temporal (Time-bound)  
    - Objetivo
    - Diccionario de Datos
    - Convenciones

3. Análisis exploratorio (EDA)
    - Importación de bibliotecas
    - Carga de datos (dataset, dataframe, base de datos, etc)
    - Exploración del conjunto de datos
        - Hallazgos
    - Tipos de Datos
        - Análisis de columnas o headers
        - Análisis variables categóricas y de tipo de datos Objeto
        - Análisis de datos faltantes o nulos
    - Descripcion estadistica
        - Análisis de outliers
        - Histogramas de outliers médicos
    - Visualización de datos
    - Analisis univariado y multivariado
    - Conclusiones

4. Modelos

5. Análisis de Componentes Principales (PCA)

6. Clusteging
  - K-Means Cluster
  - Cluster Jerárquico (Ward)

7. Bibliografia

# 1. Introducción

# **Síndrome del ovario poliquístico (PCOS)**

El síndrome de ovario poliquístico (PCOS) es un problema hormonal que afecta a las mujeres en la edad reproductiva. Aquí se presenta información relevante sobre el PCOS:

Síntomas y características del PCOS:
- **Períodos menstruales irregulares**: Las mujeres con PCOS pueden experimentar ciclos menstruales irregulares o ausencia de menstruación.
- **Exceso de vello corporal (hirsutismo)**: El PCOS puede causar un aumento en el vello facial, en el pecho, el abdomen y la espalda.
- **Acné**: Las alteraciones hormonales pueden provocar brotes de acné.
- **Calvicie**: Algunas mujeres con PCOS pueden experimentar adelgazamiento del cabello.
- **Dificultad para quedar embarazada**: El PCOS es una causa común de infertilidad.

Causas y diagnóstico:
El PCOS está relacionado con un desequilibrio hormonal y problemas metabólicos.
El diagnóstico se basa en la presencia de síntomas, análisis de sangre y ecografía para evaluar los ovarios.

Tratamiento:
El tratamiento puede incluir cambios en el estilo de vida (dieta y ejercicio), medicamentos para regular los ciclos menstruales y mejorar la fertilidad, y en algunos casos, cirugía.

# 2. Contexto

# **Pregunta inteligente (SMART)**

Que diferencias relevantes y patognomónicas existen entre las mujeres diagnosticadas con sindrome de ovario poliquistico y las que no presentan el síndrome en los 10 hospitales de Kerala, India?

- **Específico (Specific)**: Se busca saber si hay diferencias importantes entre las mujeres pacientes que se hicieron el dianostico para sindrome de ovario poliquistico en los 10 hospitales.
- **Medible (Measurable)**: A través de modelos estadísticos
- **Alcanzable (Achievable)**: La información y estudios previos permiten catalogarlo como alcanzable
- **Relevante (Relevant)**: Permitirá ayudar a comprender más y mejor el síndrome de ovario poliquístico.
- **Temporal (Time-bound)**: Se pretende estudiarlo durante 2 años

# **Objetivo**

Evaluar si se evidencian diferencias patognomónicas de acuerdo a la información recopilada en la base de datos sobre las pacientes que se hicieron el diagnóstico para el síndrome de ovario poliquistico (PCOS).

# **Diccionario de Datos**

- (s-n):	Contador de pacientes
- paciente-id:	Registro y asignación de id único y anónimo a cada paciente
- pcos(s-n):	Resultado (Si = 1, No = 0) del examen de síndrome de ovario poliquístico de la paciente
- edad-a:	Edad en años de la paciente al momento de diagnosticarse para pcos
- peso-kg:	Peso en Kilogramos de la paciente al momento de diagnosticarse para pcos
- estatura-cm:	Estatura en centimentro de la paciente al momento de diagnosticarse para pcos
- imc:	Índice de masa corporal de la paciente al momento de diagnosticarse para pcos
- grupo-sanguineo:	Factor sanguíneo RH de la paciente
- frecuencia-cardiaca-bpm:	Frecuencia cardiaca medida en pulso por minuto de la paciente al momento de diagnosticarse para pcos
- frecuencia-respiratoria-respiraciones/min:	Frecuencia respiratoria medida en respiraciones por minuto de la paciente al momento de diagnosticarse para pcos
- hemoglobina-g/dl:	Hemoglobina de la paciente expresada en gramos por decilitro al momento de diagnosticarse para pcos
- ciclo-r/i:	Duración del flujo menstrual de la paciente
- duracion-ciclo-d:	Duración del ciclo mesntrual de la paciente medido en dias
- tiempo-casada-a:	Tiempo de casada de la paciente medido en años al momento de diagnosticarse para pcos
- embarazada(s-n):	Resultado de prueba de embarazo (Si = 1, No = 0) de la paciente al momento de diagnosticarse para pcos
- nro-abortos:	Número de abortos que ha sufrido la paciente hasta el momento de diagnosticarse para pcos
- h-beta-hcg-I-mIU/mL:	Resultado de la prueba de la hormona gonadotropina coriónica humana (hCG) de la paciente al momento de diagnosticarse para pcos
- h-beta-hcg-II-mIU/mL:	Resultado de la segunda prueba de la hormona gonadotropina coriónica humana (hCG) de la paciente al momento de diagnosticarse para pcos
- h-fsh-mIU/mL:	Resultado de la prueba de la hormona foliculoestimulante (FSH) de la paciente al momento de diagnosticarse para pcos
- h-lh-mIU/mL:	Resultado de la prueba de la hormona luteinizante (LH) de la paciente al momento de diagnosticarse para pcos
- h-fsh/h-lh:	Resultado del índice del cociente entre La hormona foliculoestimulante (FSH) y la hormona luteinizante (LH) de la paciente al momento de diagnosticarse pcos
- cadera-pulg:	Medida de la cadera de la paciente en pulgadas al momento de diagnosticarse pcos
- cintura-pulg:	Medida de la cintura de la paciente en pulgadas al momento de diagnosticarse pcos
- ind-cintura/cadera:	Resultado del índice del cociente entre la medida de la cintura y la medida de la cadera de la paciente al momento de diagnosticarse pcos
- h-tsh-mIU/L:	Resultado de la prueba de la hormona estimulante de la tiroides (TSH) de la paciente al momento de diagnosticarse pcos
- h-amh-ng/mL:	Resultado de la prueba de la hormona antimülleriana (AMH) de la paciente al momento de diagnosticarse pcos
- h-prl-ng/mL:	Resultado de la hormona prolactina (PRL) de la paciente al momento de diagnosticarse pcos
- ex-vit-d3-ng/mL:	Resultado del examen de la vitamina D de la paciente al momento de diagnosticarse pcos
- h-prg-ng/mL:	Resultado de la hormona progesterona (PRG) de la paciente al momento de diagnosticarse pcos
- ex-rbs-mg/dl:	Resultado del examen del análisis de glucosa en sangre al azar (RBS) de la paciente al momento de diagnosticarse pcos
- ganancia-peso(s-n):	Respuesta (Si = 1, No = 0) de la paciente comparando su peso al momento de diagnosticarse pcos contra el control médico inmediantamente anterior
- crecimiento-cabello(s-n):	Respuesta (Si = 1, No = 0) de la paciente comparando su largo de cabello al momento de diagnosticarse pcos contra el control médico inmediantamente anterior
- oscurecimiento-piel(s-n):	Respuesta (Si = 1, No = 0) de la paciente comparando si hay oscurecimeinto de su piel al momento de diagnosticarse pcos contra el control médico inmediantamente anterior
- perdida-cabello(s-n):	Respuesta (Si = 1, No = 0) de la paciente comparando si hay pérdida de cabello al momento de diagnosticarse pcos contra el control médico inmediantamente anterior
- barro-espinilla(s-n):	Respuesta (Si = 1, No = 0) de la paciente comparando si hay aumento o aparación de espinillas, barros o granos en la piel al momento de diagnosticarse pcos contra el control médico inmediantamente anterior
- comida-rapida(s-n):	Respuesta (Si = 1, No = 0) de la paciente sobre  su alimentación con comidas rapidas o "chatarras"
- ejercicio-regular(s-n):	Respuesta (Si = 1, No = 0) de la paciente sobre  su ejercitación regular
- ps-sistolica-mmHg:	Resultado de la presión sanguinea sistólica de la paciente al momento de diagnosticarse pcos
- ps-diastolica-mmHg:	Resultado de la presión sanguinea diastólica de la paciente al momento de diagnosticarse pcos
- nro-foliculos-ovario-izq:	Número de folículos antrales en el ovario izquierdo de la paciente al momento de diagnosticarse pcos
- nro-foliculos-ovario-der:	Número de folículos antrales en el ovario derecho de la paciente al momento de diagnosticarse pcos
- prom-tam-foliculos-ovario-izq-mm:	Tamaño promedio de los folículos antrales en el ovario izquierdo de la paciente al momento de diagnosticarse pcos
- prom-tam-foliculos-ovario-der-mm:	Tamaño promedio de los folículos antrales en el ovario derecho de la paciente al momento de diagnosticarse pcos
- endometrio-mm:	Tamaño del endometrio de la paciente al momento de diagnosticarse pcos medido en milimetros

# **Convenciones**

1. **pcos** significa síndrome de ovario poliquistico
2. Por cada pregunta de respuesta tipo si-no (si = 1, no = 0), se identifica en el encabezado de cada parametro al final del nombre del mismo así: (s-n)
3. Grupo sanguíneo se define así:
(A+ = 11, A- = 12, B+ = 13, B-= 14, O+ = 15, O- = 16, AB+ = 17, AB- = 18)
4. Presión sanguínea ingresada como sistólica y diastólica de forma separada
5. Casos de la hormona Beta-HCG son mensionados como **h-beta-hcg-I-mIU/mL** y **h-beta-hcg-II-mIU/mL**
6. El prefijo **h** anterior al primer **"-"** en el nombre de cada parámetro significa hormona. Ejemplo: **h**-beta-hcg-I-mIU/mL, donde ese prefijo **h** significa hormona.
7. El prefijo **ex** anterior al primer **"-"** en el nombre de cada parámetro significa examen
8. Las unidades de medida estarán posteriores al último **"-"** en el nombre de cada parámetro y respetarán la nomenclatura médica. Ejemplo: h-beta-hcg-I-**mIU/mL**, donde su unidad de medida es: **mIU/mL**
9. Cada fila representa la información y resultados de cada paciente en el proceso de diagnosticarse para pcos

## *Dataset tomado de:*
author = {Prasoon Kottarathil},
title = {Polycystic ovary syndrome (PCOS)},
year = {2020},
publisher = {kaggle},
journal = {Kaggle Dataset},
how published = {\url{https://www.kaggle.com/prasoonkottarathil/polycystic-ovary-syndrome-pcos}}

El **Dataset** contiene todos los **parámetros físicos y clínicos** para determinar problemas relacionados con el **síndrome de ovario poliquístico (PCOS)** y la **infertilidad**. Los datos se recopilaron en **10 hospitales diferentes en Kerala, India**.

# 3. Análisis exploratorio

**Importación de bibliotecas**
"""

# Importacion de pandas y asignándole el alias pd, se usa para el tratamiento de los datos
import pandas as pd
# Importacion de numpy y asignándole el alias np, se usa para operaciones matriciales y con vectores
import numpy as np
# Importacion de matplotlib.pyplot y asignándole el alias plt, se usa para graficar
import matplotlib.pyplot as plt
# Importacion de plotly.express y asignándole el alias px, se usa para graficar
import plotly.express as px
# Importacion de seaborn y asignándole el alias sns, se usa para graficar junto a matplotlib y pandas
import seaborn as sns
# Importacion de enable_iterative_imputer de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para poder importar iterativeimputer
from sklearn.experimental import enable_iterative_imputer
# Importacion de iterativeimputer de la biblioteca sklearn, se usa para imputar datos faltantes
from sklearn.impute import IterativeImputer
# importa la clase Pipeline de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para construir flujos de trabajo
from sklearn.pipeline import Pipeline
# importa la clase StandardScaler de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para estandarizar las caracteristicas de los datos
from sklearn.preprocessing import StandardScaler
# importa la clase PCA de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para reducir dimension de los datos y encontrar caracteristicas más importantes
from sklearn.decomposition import PCA
# importa la clase LogisticRegression de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una regresión logística, herramietna para clasificacion
from sklearn.linear_model import LogisticRegression
# importa la clase KMeans de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una imputacion por clustering
from sklearn.cluster import KMeans
# importa la clase SimpleImputer de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una imputacion por clustering
from sklearn.impute import SimpleImputer
# importa la clase train_test_split de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.model_selection import train_test_split
# importa la clase RandomForestClassifier de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.ensemble import RandomForestClassifier
# importa la clase accuracy_score de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.metrics import accuracy_score
# importa la clase cross_val_score de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.model_selection import cross_val_score
# importa la clases GridSearchCV, KFold de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.model_selection import GridSearchCV, KFold
# importa la clase confusion_matrix de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.metrics import confusion_matrix
# importa la clase classification_report, ConfusionMatrixDisplay, recall_score de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.metrics import classification_report, ConfusionMatrixDisplay, recall_score
# importa la clase cross_val_predict de la biblioteca scikit-learn (también conocida como sklearn),
# se usa para hacer una modelacion
from sklearn.model_selection import cross_val_predict
# importa la clase warnings, se usa para evitar mensajes de consola en este caso apra gráficas.
import warnings

# Evitar mensajes de warning generados al mover los grid para graficas
warnings.filterwarnings('ignore')

"""**Carga de la base de datos de los pacientes**"""

# Ruta al archivo CSV (local)
#ruta_archivo = r"C:\Users\alfa7\OneDrive\Documentos\PCOS_data.csv"

# Carga los datos desde el archivo CSV su separador de columnas es una coma (,)
df = pd.read_csv('https://raw.githubusercontent.com/alfa7g7/Fundamentos-Analitica-I/main/Data/PCOS_data.csv', sep = ',')

"""**Exploración del conjunto de datos**"""

# Muestra en una tupla la cantidad de (filas, columnas) del DataFrame df
df.shape

# Muestra la información del DataFrame df, donde evidenciaremos el índice, las columnas, el tipo de dato y valores no nulos
print(df.info())

# Observamos las primeras filas del dataframe
df.head()

#Observamos las últimas filas del dataframe
df.tail()

# Muestra el nombre de las columnas (llamdas comunmente headers) del DataFrame df
print(df.columns)

"""- **Hallazgos**

    - df.shape. Nos permitió saber que nuestro dataframe tiene 44 Columnas y 541 filas
    - print(df.info()). Nos indica que nuestro dataframe tiene:
        - 19 columnas con tipo de datos flotante
        - 23 columnas con tipo de dato entero
        - 2 columnas con tipo de datos objeto
        - Determinar que los headers o nombres de cada columnas no están homogéneos, presentan espacios, etc
        - Las variables 'Marraige Status (Yrs)' y 'Fast food (Y/N)' tienen 1 dato faltante o nulo cada una.
    - print(df.columns). Nos permite observar los headers y su nombre. ( Esto debido a que debía analizarse más estos valores por lo observado con la anterior función (print(df.info())))

    Con esta información podemos determinar que se debe estandarizar las columnas o header y adicional procesar o trabajar estas columnas o variables categóricas para que podamos más adelante usar correctamente los modelos estadisticos.

**Tipos de datos**

- **Análisis de columnas o headers**
"""

# Definimos los nuevos nombres de las columnas o headers en un arreglo llamado headers
headers = ['(s-n)','paciente-id','pcos(s-n)','edad-a','peso-kg','estatura-cm','imc','grupo-sanguineo',
           'frecuencia-cardiaca-bpm','frecuencia-respiratoria-respiraciones/min','hemoglobina-g/dl',
           'ciclo-r/i','duracion-ciclo-d','tiempo-casada-a','embarazada(s-n)','nro-abortos','h-beta-hcg-I-mIU/mL',
           'h-beta-hcg-II-mIU/mL','h-fsh-mIU/mL','h-lh-mIU/mL','h-fsh/h-lh','cadera-pulg','cintura-pulg',
           'ind-cintura/cadera','h-tsh-mIU/L','h-amh-ng/mL','h-prl-ng/mL','ex-vit-d3-ng/mL','h-prg-ng/mL',
           'ex-rbs-mg/dl','ganancia-peso(s-n)','crecimiento-cabello(s-n)','oscurecimiento-piel(s-n)',
           'perdida-cabello(s-n)','barro-espinilla(s-n)','comida-rapida(s-n)','ejercicio-regular(s-n)',
           'ps-sistolica-mmHg','ps-diastolica-mmHg','nro-foliculos-ovario-izq','nro-foliculos-ovario-der',
           'prom-tam-foliculos-ovario-izq-mm','prom-tam-foliculos-ovario-der-mm','endometrio-mm']

# Asigno los nuevos nombres de encabezados o columnas a mi dataframe df
df.columns = headers

# Revisión impresa
print(df.info())

"""- **Análisis variables categóricas**

    En este caso particular solo nos interesan las dos columnas tipo objeto ya que las demás se encuentran con un tipo de dato adecuado aunque en algunas faltén algunos datos.
"""

#Primera Columna tipo objeto
df["h-beta-hcg-II-mIU/mL"].head()

#Segunda columna tipo objeto
df["h-amh-ng/mL"].head()

#Convertiremos los valores categoricos en numericos, ya que han sido numeros guardados como cadenas
#Si hay algun valor que no se puede convertir a numerico se convierte a NaN (Not a number) con error='coerce'
df["h-beta-hcg-II-mIU/mL"] = pd.to_numeric(df["h-beta-hcg-II-mIU/mL"], errors='coerce')
df["h-amh-ng/mL"] = pd.to_numeric(df["h-amh-ng/mL"], errors='coerce')

# verificacion de la conversion
df["h-beta-hcg-II-mIU/mL"].head()

# verificacion de la conversion
df["h-amh-ng/mL"].head()

# Revisión impresa, sin variables tipo objeto
print(df.info())

"""- **Análisis de datos faltantes o nulos**

"""

#Validacion de valores nulos en el dataframe muestra True quiere decir que hay valores nulos.
t = df.isnull().any().any()
print(t)

#verificando valores nulos en los parametros o columnas del dataframe si muestra True quiere decir que esa columna
#tiene valores nulos
r = df.isnull().any()
print(r)

"""Identificamos las siguientes columnas o parámetros con valores nulos:
- tiempo-casada-a
- h-beta-hcg-II-mIU/mL
- h-amh-ng/mL
- comida-rapida(s-n)

**IMPUTACION DE VALORES FALTANTES O NULOS**

La imputación de valores faltantes dio lugar a una comparación entre 2 tipos de imputación al final nos decidimos por la imputación múltiple por ecuaciones encadenadas (MICE) debido a que:

- El algoritmo MICE asume que los datos faltan al azar (MAR), es decir, la falta de un campo se puede explicar por los valores de otras columnas, pero no de esa columna

**MICE**

    Verificación de valores faltantes y llenado de datos faltantes o nulos en los parámetros o columnas del dataframe con imputacion MICE
"""

# Imputacion de datos por MICE
# Inicializa el imputador MICE
mice_imputer = IterativeImputer(max_iter=10, random_state=0)
# Realiza la imputación
df_imputed = mice_imputer.fit_transform(df)
# El resultado es un array NumPy, así que puedes convertirlo de nuevo a DataFrame
df_imputed = pd.DataFrame(df_imputed, columns=df.columns)
#df = df_imputed
print(df_imputed.info())

# Cambio las variables con datos ya imputados solamente al dataframe original para que mis variables no cambien todas a tipo flotante
df['tiempo-casada-a'] = df_imputed['tiempo-casada-a']
df['h-beta-hcg-II-mIU/mL'] = df_imputed['h-beta-hcg-II-mIU/mL']
df['h-amh-ng/mL'] = df_imputed['h-amh-ng/mL']
# redondeamos la siguiente variable imputada para evitarun error en su respuesta.
# (Corrección hecha después de evaluación gráfica inicial)
df_imputed['comida-rapida(s-n)'] = df_imputed['comida-rapida(s-n)'].round()
df['comida-rapida(s-n)'] = df_imputed['comida-rapida(s-n)']

df.info()

df['(s-n)'].nunique()

df['paciente-id'].nunique()

"""Observando la estructura del dataframe procedemos a sacar esas dos variables ya que ambas son identificadores tipo contador para enamurar cada uno de los pacientes del estudio, probablemente usadas incialmente para encriptar o enmascarar los datos sensibles de cada paciente (Nombres o identificacion)."""

df.drop(columns = ['(s-n)', 'paciente-id'], axis =1, inplace = True)

df.info()

#verificando valores nulos en los parametros o columnas del dataframe después de la imputacion por MICE
r = df.isnull().any()
print(r)

#Validacion de que no queden valores nulos en el dataframe Si muestra False quiere decir que ya no hay nulos
t = df.isnull().any().any()
print(t)

"""**Descripción estadística**"""

# Resumen estadístico del dataframe df
df.describe()

"""- **Análisis de outliers**"""

df.max()

px.box(df)

"""Análisis gráfico variables categóricas"""

# Gráfico para las variables categoricas SI = 1 - NO = 0
px.box(df, y = ['pcos(s-n)', 'embarazada(s-n)', 'ganancia-peso(s-n)', 'crecimiento-cabello(s-n)',
                'oscurecimiento-piel(s-n)', 'perdida-cabello(s-n)', 'barro-espinilla(s-n)',
                'comida-rapida(s-n)', 'ejercicio-regular(s-n)'])

px.histogram(df['pcos(s-n)'])

px.histogram(df['embarazada(s-n)'])

px.histogram(df['crecimiento-cabello(s-n)'])

px.histogram(df['oscurecimiento-piel(s-n)'])

px.histogram(df['perdida-cabello(s-n)'])

px.histogram(df['ganancia-peso(s-n)'])

px.histogram(df['barro-espinilla(s-n)'])

px.histogram(df['comida-rapida(s-n)'])

px.histogram(df['ejercicio-regular(s-n)'])

"""Análisis gráfico variables no categóricas"""

px.box(df['edad-a'], points='all')

px.box(df['peso-kg'], points='all')

px.box(df['estatura-cm'], points='all')

px.box(df['imc'], points='all')

px.box(df['grupo-sanguineo'], points='all')

px.box(df['frecuencia-cardiaca-bpm'], points='all')

px.box(df['frecuencia-respiratoria-respiraciones/min'], points='all')

px.box(df['hemoglobina-g/dl'], points='all')

px.box(df['ciclo-r/i'], points='all')

px.histogram(df['ciclo-r/i'])

px.box(df['duracion-ciclo-d'], points='all')

px.box(df['tiempo-casada-a'], points='all')

px.box(df['nro-abortos'], points='all')

px.histogram(df['nro-abortos'])

px.box(df['h-beta-hcg-I-mIU/mL'], points='all')

px.histogram(df['h-beta-hcg-I-mIU/mL'])

px.box(df['h-beta-hcg-II-mIU/mL'], points='all')

px.histogram(df['h-beta-hcg-II-mIU/mL'])

px.box(df['h-fsh-mIU/mL'], points='all')

px.histogram(df['h-fsh-mIU/mL'])

px.box(df['h-lh-mIU/mL'], points='all')

px.box(df['h-fsh/h-lh'], points='all')

px.box(df['cadera-pulg'], points='all')

px.box(df['cintura-pulg'], points='all')

px.box(df['ind-cintura/cadera'], points='all')

px.box(df['h-tsh-mIU/L'], points='all')

px.box(df['h-amh-ng/mL'], points='all')

px.box(df['h-prl-ng/mL'], points='all')

px.box(df['ex-vit-d3-ng/mL'], points='all')

px.box(df['h-prg-ng/mL'], points='all')

px.box(df['ex-rbs-mg/dl'], points='all')

px.box(df['ps-sistolica-mmHg'], points='all')

px.box(df['ps-diastolica-mmHg'], points='all')

px.box(df['nro-foliculos-ovario-izq'], points='all')

px.box(df['nro-foliculos-ovario-der'], points='all')

px.box(df['prom-tam-foliculos-ovario-izq-mm'], points='all')

px.box(df['prom-tam-foliculos-ovario-der-mm'], points='all')

px.box(df['endometrio-mm'], points='all')

#Histograma de variable determinada para remover outliers
figura1 = px.histogram(df, x='ps-diastolica-mmHg', nbins=30)
figura1.show()

#Histograma de variable determinada para remover outliers
figura2 = px.histogram(df, x='ps-sistolica-mmHg', nbins=30)
figura2.show()

#Histograma de variable determinada para remover outliers
figura3 = px.histogram(df, x='endometrio-mm', nbins=30)
figura3.show()

#Histograma de variable determinada para remover outliers
figura4 = px.histogram(df, x='prom-tam-foliculos-ovario-izq-mm', nbins=60)
figura4.show()

#Histograma de variable determinada para remover outliers
figura5 = px.histogram(df, x='prom-tam-foliculos-ovario-der-mm', nbins=60)
figura5.show()

#Histograma de variable determinada para remover outliers
figura6 = px.histogram(df, x='frecuencia-cardiaca-bpm', nbins=30)
figura6.show()

# Teniendo en cuenta el concepto medico procedemos a identificar y sacar datos outliers
# Aquí discrepamos y preferimos mantener ciertos valores que resultan mas reales y significativos
# a largo plazo en algunos outliers propuestos por la autora original del estudio.

df = df[(df['ps-diastolica-mmHg']>20)]
df = df[(df['ps-sistolica-mmHg']>20)]
df = df[(df['endometrio-mm']>0)]
df = df[(df['prom-tam-foliculos-ovario-izq-mm']>0)]
df = df[(df['prom-tam-foliculos-ovario-der-mm']>0)]
df = df[(df['frecuencia-cardiaca-bpm']>20)]
df = df[(df['ciclo-r/i']<4.5)]

df.shape

"""Los motivos de exclusion para estos registros fueron tomados teniendo en cuenta el concepto de los expecialistas en el área específica de conocimiento. Según concepto de ellos:

- las presiones diastólicas, sistólicas y la frecuencia cardiaca inferiores en valor a 20 con incompatibles con la vida.

- Un endometrio con un tamaño menor a cero es un error de digitación por parte del personal encargado de generear estos informes inicialmente, teniendo en cuenta que estos alimentaron el dataframe para el estudio.

- El tamaño de los folículos en ambos ovarios si y solo sí, debe ser un valor positivo; en cualquier escenario contrario esto significa en error de llenado por parte del personal humano a la hora de rendir el informe con el cual se llenó el dataframe.

- En cuestion a los ciclos se determinó que era un varialble dicotómica por ende cualquier respues que estuviera fuera de su rango es decir un valor mayor a 4, significa o da a entender nuevamente un error de digitación por ende esos mismos son excluidos.

Estos criterios de exclusión fueron tenidos en cuenta porteriormentes a una reunión con especialistas en el dominio del tema.

Como se muestra a continuación ya el valor mínimo es superior a cero en todos los parámetros excepto los que originalmente eran categóricos con respuesta (s-n), para los cuales no aplica el criterio.
"""

df.describe()

# Diagrama de caja de la variable sin outliers
px.box(df['ps-diastolica-mmHg'], points='all')

#Histograma de variable sin el outlier
px.histogram(df, x='ps-diastolica-mmHg', nbins=30)

# Diagrama de caja de la variable sin outliers
px.box(df['ps-sistolica-mmHg'], points='all')

#Histograma de variable sin outliers
px.histogram(df, x='ps-sistolica-mmHg', nbins=30)

# Diagrama de caja de la variable sin outliers
px.box(df['endometrio-mm'], points='all')

#Histograma de variable sin outliers
px.histogram(df, x='endometrio-mm', nbins=30)

# Diagrama de caja de la variable sin outliers
px.box(df['prom-tam-foliculos-ovario-izq-mm'], points='all')

#Histograma de variable sin outliers
figura10 = px.histogram(df, x='prom-tam-foliculos-ovario-izq-mm', nbins=60)
figura10.show()

# Diagrama de caja de la variable sin outliers
px.box(df['prom-tam-foliculos-ovario-der-mm'], points='all')

#Histograma de variable sin outliers
figura11 = px.histogram(df, x='prom-tam-foliculos-ovario-der-mm', nbins=60)
figura11.show()

# Diagrama de caja de la variable sin outliers
px.box(df['frecuencia-cardiaca-bpm'], points='all')

#Histograma de variable sin outliers
figura12 = px.histogram(df, x='frecuencia-cardiaca-bpm', nbins=30)
figura12.show()

# Grafico de caja sin outlier para ciclo regular o irregular
px.box(df['ciclo-r/i'], points='all')

#Histograma de variable sin outliers
px.histogram(df['ciclo-r/i'])

"""**Visualizacion de datos**"""

#Relacion entre las variables
sns.pairplot(df)

#Determinamos una matriz de correlación de todos los valores de cada parametro

# sacar las variables categoricas del grafico de correlacion.
corrmat = df.corr()
plt.subplots(figsize=(18,18))
sns.heatmap(corrmat,cmap="Pastel1", square=True);

"""**Analisis univariado y multivariado**"""

# Determinamos el valor de correlacion de todos los parametros para el parametro pcos(s-n)
# desde el más significativo al menos
corrmat = df.corr()
corrmat['pcos(s-n)'].sort_values(ascending=False)

"""Podemos observar 29 variables con valor positivo y 13 con valores negativos.
Aquí es importante resaltar que al tratarse de un estudio de un caso médico específico decidimos medir corralaciones en variables categóricas aunque teoricamente no se debería incluir.
"""

#Bastantes variables tienen correlación significativa

plt.figure(figsize=(18,18))
k = 29 #número de variables con mapa de calor positivo
l = 3 #número de variables con mapa de calor negativo
cols_p = corrmat.nlargest(k, "pcos(s-n)")["pcos(s-n)"].index
cols_n = corrmat.nsmallest(l, "pcos(s-n)")["pcos(s-n)"].index
cols = cols_p.append(cols_n)

cm = np.corrcoef(df[cols].values.T)
sns.set(font_scale=1.25)
hm = sns.heatmap(cm, cbar=True,cmap="Pastel1", annot=True, square=True, fmt='.2f',
                 annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()

#Duración de la fase menstrual en sindrome de ovario poliquistivo vs normal
color = ["teal", "plum"]
sns.lmplot(data=df,x="edad-a",y="duracion-ciclo-d", hue="pcos(s-n)",palette=color)
plt.show()

# Patrón de aumento de peso (IMC) a lo largo de los años en pacientes con pcos y normal.
sns.lmplot(data =df,x="edad-a",y="imc", hue="pcos(s-n)", palette= color )
plt.show()

# Patron del ciclo menstrual (r/i) con la edad y el pcos
sns.lmplot(data =df,x="edad-a",y="ciclo-r/i", hue="pcos(s-n)",palette=color)
plt.show()

"""Aunque no es una convencíon después del análisis de evidencia que probablemente el tipo de respuesta igual a 2 equivale a un ciclo irregular y 4 a un ciclo regular.

ciclo-r/i = 2 para ciclos irregulares
ciclo-r/i = 4 para ciclos regulares

"""

# Distribución de folicutlos en ambos ovarios
sns.lmplot(data =df,x='nro-foliculos-ovario-izq',y='nro-foliculos-ovario-der',
           hue='pcos(s-n)',palette=color)
plt.show()

caracteristicas_fol = ["nro-foliculos-ovario-izq","nro-foliculos-ovario-der"]
for i in caracteristicas_fol:
    sns.swarmplot(x=df["pcos(s-n)"], y=df[i], color="black", alpha=0.5 )
    sns.boxenplot(x=df["pcos(s-n)"], y=df[i], palette=color)
    plt.show()

caracteristicas = ['edad-a', 'peso-kg', 'imc', 'hemoglobina-g/dl', 'duracion-ciclo-d', 'endometrio-mm']
for i in caracteristicas:
    sns.swarmplot(x=df['pcos(s-n)'], y=df[i], color="black", alpha=0.5 )
    sns.boxenplot(x=df['pcos(s-n)'], y=df[i], palette=color)
    plt.show()

"""**Conclusiones**

La duración de la fase menstrual es, en general, constante en diferentes edades en los casos normales. Mientras que en el caso del PCOS la duración aumentó con la edad.

El índice de masa corporal (IMC) muestra consistencia en los casos normales. Mientras que en el caso del síndrome de ovario poliquístico, el IMC aumenta con la edad.

El ciclo menstrual se vuelve más regular en los casos normales con la edad. Mientras que en el caso del síndrome de ovario poliquístico la irregularidad aumenta con la edad.

La distribución de los folículos en ambos ovarios, izquierdo y derecho, no es igual para las mujeres con síndrome de ovario poliquístico en comparación con la paciente "normal".

La cantidad de folículos en mujeres con pcos es mayor, como se esperaba. Y también son desiguales.

La muestra es corta para determinar si hay hechos patognómonicas que arrojen nueva información a los estudios y conociemientos actuales. Se recomienda aumentar la muestra.

###4. Modelado

<html lang="es">

<body>
    <h2>Evaluación de Modelos de Clasificación para Predicción del Síndrome de Ovarios Poliquísticos</h2>
    <p>
        Para abordar el objetivo de este ejercicio, que consiste en desarrollar un modelo capaz de predecir si una mujer es propensa a padecer síndrome de ovarios poliquísticos, se evaluarán inicialmente un total de ocho modelos de clasificación. Estos modelos son:
    </p>
    <ul>
        <li>Regresión Logística</li>
        <li>KNN</li>
        <li>Naive Bayes</li>
        <li>Árboles de Decisión con criterio ID3</li>
        <li>Árboles de Decisión con criterio CART</li>
        <li>Bagging</li>
        <li>Random Forest</li>
        <li>XGBoost</li>
    </ul>
    <h2>Escalado de los Datos</h2>
    <p>
            Dadas las diferentes escalas de las variables, se opta por realizar una estandarización de los datos. Este proceso escrucial para garantizar que todas las variables contribuyan de manera equitativa al modelo, evitando que aquellas con mayor magnitud dominen el proceso de aprendizaje. El escalado de datos mejora la eficiencia y el rendimiento del modelo, asegurando una mejor convergencia durante el entrenamiento y permitiendo una comparación más justa entre las diferentes variables.
    </p>
    <h2>Métricas Clave</h2>
    <p>
        <strong>Accuracy:</strong> Esta métrica permite determinar la precisión del modelo en términos de aciertos tanto en mujeres que tienen como en las que no tienen el síndrome de ovarios poliquísticos.
    </p>
    <p>
        <strong>Recall:</strong> Dado que es crucial minimizar la cantidad de falsos negativos (mujeres a las que se les predice que no tienen el síndrome cuando en realidad sí lo tienen), el recall es esencial para evaluar la capacidad del modelo para identificar correctamente los casos positivos.
    </p>
    <h2>Protocolos de Evaluación</h2>
    <p>
        Para obtener una evaluación más completa y llegar al modelo óptimo, cada uno de los modelos se someterá a dos fases de análisis:
    </p>
    <ol>
        <li><strong>Fase sin búsqueda de hiperparámetros:</strong> En esta fase inicial, los modelos se evaluarán con sus configuraciones por defecto, haciendo uso de validación cruzada.</li>
        <li><strong>Fase con búsqueda de hiperparámetros:</strong> En la segunda fase, se realizará una optimización de hiperparámetros utilizando los protocolos de validación cruzada K-fold y GridSearchCV.</li>
    </ol>
    <p>Para cada modelo se obtendrá el promedio de accuracy y recall obtenidos en las validaciones cruzadas, y serán estas métricas las que se tendrán en cuenta para el análisis de cada modelo. Se genera tambien para cada modelo la matriz de confusión corriendo el modelo con los datos de entrenamiento.
    </p>
    <h2>Esquema del Proceso</h2>
    <p>El esquema gráfico del proceso es el siguiente:</p>
    <img src="https://raw.githubusercontent.com/leoe21/images/main/image%20cv.png" alt="Esquema de evaluación de modelos" style="width:100%;max-width:600px;">
    <h2>Selección del Modelo</h2>
    <ol>
        <li><strong>Selección del modelo:</strong> Evaluar todos los modelos listados y calcular las métricas respectivas (accuracy y recall).</li>
        <li><strong>Optimización del modelo seleccionado:</strong> Seleccionar el modelo con mejor rendimiento y realizar una búsqueda de hiperparámetros más exhaustiva utilizando GridSearchCV y K-fold cross validation para obtener el modelo definitivo.</li>
    </ol>
    <p>
        Este enfoque nos permitirá identificar el modelo con mejor desempeño en la predicción del síndrome de ovarios poliquísticos, asegurando un equilibrio entre precisión y capacidad para detectar casos positivos.
    </p>
</body>
</html>
"""

# Codificando en 1 y 0 ciclo regular
df["ciclo-r/i"]=df["ciclo-r/i"].replace({4:1,2:0})
df.rename(columns={"ciclo-r/i":"ciclo-r"}, inplace=True)

"""### **Partición del Dataset y escalado de los datos**"""

# Preparar los datos
# Separar las características (X) de la variable objetivo (y)
X = df.drop(columns=['pcos(s-n)'])
y = df['pcos(s-n)']

# Escalar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Dividir el dataset en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

"""### **Protocolo de evaluación sobre datos de entrenamiento**"""

kfold = KFold(n_splits=20, shuffle=True, random_state=42)

"""Este código configura una validación cruzada de 20 folds con barajado de los datos, utilizando una semilla para asegurar la reproducibilidad. Esto será útil para evaluar el rendimiento de los siguientes modelos de manera más robusta, ya que se entrena y evalúa el modelo en diferentes particiones del conjunto de datos, reduciendo el riesgo de sobreajuste y proporcionando una mejor estimación del rendimiento del modelo en datos no vistos.

<strong>Modelo 1: Regresión Logística</strong>
<br><br>
Presentamos versiones de cada modelo, uno sin busqueda de hiérparametros y otro con busqueda.

<strong>Regresión Logística sin busqueda de hiperperparametros óptimos</strong>
<br><br>
"""

###Modelo de regresión Logisitica

# Entrenar el modelo de regresión logística
model_rl = LogisticRegression(max_iter=5000, solver='lbfgs')
model_rl.fit(X_train, y_train)

# Realizar validación cruzada para accuracy
cv_accuracy_rl = cross_val_score(model_rl, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_rl = cv_accuracy_rl.mean()

# Realizar validación cruzada para recall
y_pred_cv_rl = cross_val_predict(model_rl, X_train, y_train, cv=kfold)
cv_recall_rl = recall_score(y_train, y_pred_cv_rl, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_rl))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_rl))

#Matriz de consufión para data train
y_train_pred_rl = model_rl.predict(X_train)
conf_matrix_train_rl = confusion_matrix(y_train, y_train_pred_rl)


# Reporte de clasificación
class_report_train_rl = classification_report(y_train, y_train_pred_rl)
print('Reporte con datos de train:')
print(class_report_train_rl)


# Crear Matriz
sns.heatmap(conf_matrix_train_rl, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<br><br>
<strong>Regresión Logística con busqueda de hiperperparametros óptimos</strong>
"""

# Definir los hiperparámetros a ajustar
#Definir los parametros para optimizar

param_grid_rl = {
    'C': [0.9, 1.0, 1.1],
    'penalty': ['elasticnet'],  # Usar Elastic Net
    'l1_ratio': [0.1, 0.5, 0.9]
}


# Inicializar el GridSearchCV
grid_search_rl = GridSearchCV(estimator=LogisticRegression(max_iter=5000, solver='saga'),
                           param_grid=param_grid_rl,
                           cv=kfold,
                           scoring='accuracy',
                           verbose=1,
                           n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_rl.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params = grid_search_rl.best_params_
print("Mejores parámetros:", best_params)

# Mejor modelo
best_model_rl = grid_search_rl.best_estimator_



# Realizar validación cruzada para accuracy
cv_accuracy_rl_2 = cross_val_score(best_model_rl, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_rl_2 = cv_accuracy_rl_2.mean()

# Realizar validación cruzada para recall
y_pred_cv_rl_2 = cross_val_predict(best_model_rl, X_train, y_train, cv=kfold)
cv_recall_rl_2 = recall_score(y_train, y_pred_cv_rl_2, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_rl_2))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_rl_2))

#Matriz de consufión para data train
y_train_pred_rl_2 = best_model_rl.predict(X_train)
conf_matrix_train_rl_2 = confusion_matrix(y_train, y_train_pred_rl_2)


# Reporte de clasificación
class_report_train_rl_2 = classification_report(y_train, y_train_pred_rl_2)
print('Reporte con datos de train:')
print(class_report_train_rl_2)


# Crear Matriz
sns.heatmap(conf_matrix_train_rl_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Modelo 2: K vecinos más cercanos</strong>

<strong>K vecinos más cercanos sin busqueda de hiperparametros</strong>
"""

from sklearn.neighbors import KNeighborsClassifier

#Modelo KNN

k = 5
model_knn = KNeighborsClassifier(n_neighbors=k)
model_knn.fit(X_train, y_train)

# Realizar validación cruzada para accuracy
cv_accuracy_knn = cross_val_score(model_knn, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_knn = cv_accuracy_knn.mean()

# Realizar validación cruzada para recall
y_pred_cv_knn = cross_val_predict(model_knn, X_train, y_train, cv=kfold)
cv_recall_knn = recall_score(y_train, y_pred_cv_knn, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_knn))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_knn))

#Matriz de consufión para data train
y_train_pred_knn = model_knn.predict(X_train)
conf_matrix_train_knn = confusion_matrix(y_train, y_train_pred_knn)


# Reporte de clasificación
class_report_train_knn = classification_report(y_train, y_train_pred_knn)
print('Reporte con datos de train:')
print(class_report_train_knn)


# Crear Matriz
sns.heatmap(conf_matrix_train_knn, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>K vecinos más cercanos con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros a ajustar

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11],  # Número de vecinos
    'metric': ['euclidean', 'manhattan']  # Test distancias
}


# Inicializar el GridSearchCV
grid_search_knn = GridSearchCV(estimator=KNeighborsClassifier(),
                           param_grid=param_grid_knn,
                           cv=kfold,
                           scoring='accuracy',
                           verbose=1,
                           n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_knn.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_knn = grid_search_knn.best_params_
print("Mejores parámetros:", best_params_knn)

# Mejor modelo
best_model_knn = grid_search_knn.best_estimator_



# Realizar validación cruzada para accuracy
cv_accuracy_knn_2 = cross_val_score(best_model_knn, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_knn_2 = cv_accuracy_knn_2.mean()

# Realizar validación cruzada para recall
y_pred_cv_knn_2 = cross_val_predict(best_model_knn, X_train, y_train, cv=kfold)
cv_recall_knn_2 = recall_score(y_train, y_pred_cv_knn_2, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_knn_2))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_knn_2))

#Matriz de consufión para data train
y_train_pred_knn_2 = best_model_knn.predict(X_train)
conf_matrix_train_knn_2 = confusion_matrix(y_train, y_train_pred_knn_2)


# Reporte de clasificación
class_report_train_knn_2 = classification_report(y_train, y_train_pred_knn_2)
print('Reporte con datos de train:')
print(class_report_train_knn_2)


# Crear Matriz
sns.heatmap(conf_matrix_train_knn_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Modelo 3: Naive Bayes</strong>"""

from sklearn.naive_bayes import GaussianNB
# Modelo Naive Bayes

# Entrenar el modelo Naive Bayes

model_nb = GaussianNB()
model_nb.fit(X_train, y_train)


# Realizar validación cruzada para accuracy
cv_accuracy_nb = cross_val_score(model_nb, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_nb = cv_accuracy_nb.mean()

# Realizar validación cruzada para recall
y_pred_cv_nb = cross_val_predict(model_nb, X_train, y_train, cv=kfold)
cv_recall_nb = recall_score(y_train, y_pred_cv_nb, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_nb))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_nb))

#Matriz de consufión para data train
y_train_pred_nb = model_nb.predict(X_train)
conf_matrix_train_nb = confusion_matrix(y_train, y_train_pred_nb)


# Reporte de clasificación
class_report_train_nb = classification_report(y_train, y_train_pred_nb)
print('Reporte con datos de train:')
print(class_report_train_nb)


# Crear Matriz
sns.heatmap(conf_matrix_train_nb, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Modelo 4: Arboles de decisión con criterio ID3</strong>

<strong>Arboles de decisión con criterio ID3 sin busqueda de hiperparametros</strong>
"""

from sklearn.tree import DecisionTreeClassifier

### Arboles de desicion con criteriod ID3 (Entropia)

# Entrenar el modelo de árbol de decisión usando el criterio "entropy" (ID3)

model_id3 = DecisionTreeClassifier(criterion='entropy', random_state=42)
model_id3.fit(X_train, y_train)


# Realizar validación cruzada para accuracy
cv_accuracy_id3 = cross_val_score(model_id3, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_id3 = cv_accuracy_id3.mean()

# Realizar validación cruzada para recall
y_pred_cv_id3 = cross_val_predict(model_id3, X_train, y_train, cv=kfold)
cv_recall_id3 = recall_score(y_train, y_pred_cv_id3, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_id3))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_id3))

#Matriz de consufión para data train
y_train_pred_id3 = model_id3.predict(X_train)
conf_matrix_train_id3 = confusion_matrix(y_train, y_train_pred_id3)


# Reporte de clasificación
class_report_train_id3 = classification_report(y_train, y_train_pred_id3)
print('Reporte con datos de train:')
print(class_report_train_id3)


# Crear Matriz
sns.heatmap(conf_matrix_train_id3, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Arboles de decisión con criterio ID3 con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros a ajustar

param_grid_id3 = {
    'max_depth': [None, 5, 10, 15],  # Vary the maximum depth of the tree
    'min_samples_split': [2, 5, 10],  # Vary the minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]  # Vary the minimum number of samples required to be at a leaf node
}


# Inicializar el GridSearchCV
grid_search_id3 = GridSearchCV(estimator=DecisionTreeClassifier(criterion='entropy', random_state=42),
                           param_grid=param_grid_id3,
                           cv=kfold,
                           scoring='accuracy',
                           verbose=1,
                           n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_id3.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_id3 = grid_search_id3.best_params_
print("Mejores parámetros:", best_params_id3)

# Mejor modelo
best_model_id3 = grid_search_id3.best_estimator_


# Realizar validación cruzada para accuracy
cv_accuracy_id3_2 = cross_val_score(best_model_id3, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_id3_2 = cv_accuracy_id3_2.mean()

# Realizar validación cruzada para recall
y_pred_cv_id3_2 = cross_val_predict(best_model_id3, X_train, y_train, cv=kfold)
cv_recall_id3_2 = recall_score(y_train, y_pred_cv_id3_2, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_id3_2))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_id3_2))

#Matriz de consufión para data train
y_train_pred_id3_2 = best_model_id3.predict(X_train)
conf_matrix_train_id3_2 = confusion_matrix(y_train, y_train_pred_id3_2)


# Reporte de clasificación
class_report_train_id3_2 = classification_report(y_train, y_train_pred_id3_2)
print('Reporte con datos de train:')
print(class_report_train_id3_2)


# Crear Matriz
sns.heatmap(conf_matrix_train_id3_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Modelo 5: Arboles de decisión con criterio CART</strong>

<strong>Arboles de decisión con criterio CART sin busqueda de hiperparametros</strong>
"""

# Entrenar el modelo de árbol de decisión usando el criterio "gini" (CART)

model_cart = DecisionTreeClassifier(criterion='gini', random_state=42)
model_cart.fit(X_train, y_train)


# Realizar validación cruzada para accuracy
cv_accuracy_cart = cross_val_score(model_cart, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_cart = cv_accuracy_cart.mean()

# Realizar validación cruzada para recall
y_pred_cv_cart = cross_val_predict(model_cart, X_train, y_train, cv=kfold)
cv_recall_cart = recall_score(y_train, y_pred_cv_cart, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_cart))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_cart))

#Matriz de consufión para data train
y_train_pred_cart = model_cart.predict(X_train)
conf_matrix_train_cart = confusion_matrix(y_train, y_train_pred_cart)


# Reporte de clasificación
class_report_train_cart = classification_report(y_train, y_train_pred_cart)
print('Reporte con datos de train:')
print(class_report_train_cart)


# Crear Matriz
sns.heatmap(conf_matrix_train_cart, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Arboles de decisión con criterio CART con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros a ajustar

param_grid_cart = {
    'max_depth': [None, 5, 10, 15],  # Vary the maximum depth of the tree
    'min_samples_split': [2, 5, 10],  # Vary the minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]  # Vary the minimum number of samples required to be at a leaf node
}


# Inicializar el GridSearchCV
grid_search_cart = GridSearchCV(estimator=DecisionTreeClassifier(criterion='gini', random_state=42),
                           param_grid=param_grid_cart,
                           cv=kfold,
                           scoring='accuracy',
                           verbose=1,
                           n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_cart.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_cart = grid_search_cart.best_params_
print("Mejores parámetros:", best_params_cart)

# Mejor modelo
best_model_cart = grid_search_cart.best_estimator_



# Realizar validación cruzada para accuracy
cv_accuracy_cart_2 = cross_val_score(best_model_cart, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_cart_2 = cv_accuracy_cart_2.mean()

# Realizar validación cruzada para recall
y_pred_cv_cart_2 = cross_val_predict(best_model_cart, X_train, y_train, cv=kfold)
cv_recall_cart_2 = recall_score(y_train, y_pred_cv_cart_2, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_cart_2))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_cart_2))

#Matriz de consufión para data train
y_train_pred_cart_2 = best_model_cart.predict(X_train)
conf_matrix_train_cart_2 = confusion_matrix(y_train, y_train_pred_cart_2)


# Reporte de clasificación
class_report_train_cart_2 = classification_report(y_train, y_train_pred_cart_2)
print('Reporte con datos de train:')
print(class_report_train_cart_2)


# Crear Matriz
sns.heatmap(conf_matrix_train_cart_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Modelo 6: Bagging</strong>

<strong>Bagging sin busqueda de hiperparametros</strong>
"""

from sklearn.ensemble import BaggingClassifier

# Modelo Baggin (usando como base estimador de gini)
# Entrenar el modelo Bagging con un árbol de decisión

base_estimator = DecisionTreeClassifier(criterion='entropy', random_state=42)
model_bag = BaggingClassifier(estimator=base_estimator, n_estimators=100, random_state=42)
model_bag.fit(X_train, y_train)

# Realizar validación cruzada para accuracy
cv_accuracy_bag = cross_val_score(model_bag, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_bag = cv_accuracy_bag.mean()

# Realizar validación cruzada para recall
y_pred_cv_bag = cross_val_predict(model_bag, X_train, y_train, cv=kfold)
cv_recall_bag = recall_score(y_train, y_pred_cv_bag, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_bag))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_bag))

#Matriz de consufión para data train
y_train_pred_bag = model_bag.predict(X_train)
conf_matrix_train_bag = confusion_matrix(y_train, y_train_pred_bag)


# Reporte de clasificación
class_report_train_bag = classification_report(y_train, y_train_pred_bag)
print('Reporte con datos de train:')
print(class_report_train_bag)

# Crear Matriz
sns.heatmap(conf_matrix_train_bag, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Bagging con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros para el árbol de decisión

param_grid_bag = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}


# Inicializar GridSearchCV para el árbol de decisión
grid_search_bag = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),
                              param_grid=param_grid_bag,
                              cv=kfold,
                              scoring='accuracy',
                              verbose=1,
                              n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_bag.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_bag = grid_search_bag.best_params_
print("Mejores parámetros:", best_params_bag)

# Mejor modelo
best_model_bag = grid_search_bag.best_estimator_

# Realizar validación cruzada para accuracy
cv_accuracy_bag_2 = cross_val_score(best_model_bag, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_bag_2 = cv_accuracy_bag_2.mean()

# Realizar validación cruzada para recall
y_pred_cv_bag_2 = cross_val_predict(best_model_bag, X_train, y_train, cv=kfold)
cv_recall_bag_2 = recall_score(y_train, y_pred_cv_bag_2, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_bag_2))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_bag_2))

#Matriz de consufión para data train
y_train_pred_bag_2 = best_model_bag.predict(X_train)
conf_matrix_train_bag_2 = confusion_matrix(y_train, y_train_pred_bag_2)


# Reporte de clasificación
class_report_train_bag_2 = classification_report(y_train, y_train_pred_bag_2)
print('Reporte con datos de train:')
print(class_report_train_bag_2)


# Crear Matriz
sns.heatmap(conf_matrix_train_bag_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Modelo 7: Random Forest</strong>

<strong> Random Forest sin busqueda de hiperparametros</strong>
"""

from sklearn.ensemble import RandomForestClassifier

#Modelo Random Forest

# Entrenar el modelo Random Forest
model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)


# Realizar validación cruzada para accuracy
cv_accuracy_rf = cross_val_score(model_rf, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_rf = cv_accuracy_rf.mean()

# Realizar validación cruzada para recall
y_pred_cv_rf = cross_val_predict(model_rf, X_train, y_train, cv=kfold)
cv_recall_rf = recall_score(y_train, y_pred_cv_rf, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_rf))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_rf))

#Matriz de consufión para data train
y_train_pred_rf = model_rf.predict(X_train)
conf_matrix_train_rf = confusion_matrix(y_train, y_train_pred_rf)


# Reporte de clasificación
class_report_train_rf = classification_report(y_train, y_train_pred_rf)
print('Reporte con datos de train:')
print(class_report_train_rf)


# Crear Matriz
sns.heatmap(conf_matrix_train_rf, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong> Random Forest con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros a ajustar

param_grid_rf = {
'n_estimators': [50, 100, 150],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}


# Inicializar GridSearchCV para el Random Forest
grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                              param_grid=param_grid_rf,
                              cv=kfold,
                              scoring='accuracy',
                              verbose=1,
                              n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_rf.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_rf = grid_search_rf.best_params_
print("Mejores parámetros:", best_params_rf)

# Mejor modelo
best_model_rf = grid_search_rf.best_estimator_



# Realizar validación cruzada para accuracy
cv_accuracy_rf_2 = cross_val_score(best_model_rf, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_rf_2 = cv_accuracy_rf_2.mean()

# Realizar validación cruzada para recall
y_pred_cv_rf_2 = cross_val_predict(best_model_rf, X_train, y_train, cv=kfold)
cv_recall_rf_2 = recall_score(y_train, y_pred_cv_rf_2, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_rf_2))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_rf_2))

#Matriz de consufión para data train
y_train_pred_rf_2 = best_model_rf.predict(X_train)
conf_matrix_train_rf_2 = confusion_matrix(y_train, y_train_pred_rf_2)


# Reporte de clasificación
class_report_train_rf_2 = classification_report(y_train, y_train_pred_rf_2)
print('Reporte con datos de train:')
print(class_report_train_rf_2)


# Crear Matriz
sns.heatmap(conf_matrix_train_rf_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>Modelo 8: XGBoost</strong>

# <strong>XGBoost sin busqueda de hiperparametros</strong>
"""

import xgboost as xgb

# XGBoost

# Entrenar el modelo XGBoost
model_xgb = xgb.XGBClassifier(objective="binary:logistic", random_state=42)
model_xgb.fit(X_train, y_train)



# Realizar validación cruzada para accuracy
cv_accuracy_xgb = cross_val_score(model_xgb, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_xgb = cv_accuracy_xgb.mean()

# Realizar validación cruzada para recall
y_pred_cv_xgb = cross_val_predict(model_xgb, X_train, y_train, cv=kfold)
cv_recall_xgb = recall_score(y_train, y_pred_cv_xgb, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_xgb))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_xgb))

#Matriz de consufión para data train
y_train_pred_xgb = model_xgb.predict(X_train)
conf_matrix_train_xgb = confusion_matrix(y_train, y_train_pred_xgb)


# Reporte de clasificación
class_report_train_xgb = classification_report(y_train, y_train_pred_xgb)
print('Reporte con datos de train:')
print(class_report_train_xgb)


# Crear Matriz
sns.heatmap(conf_matrix_train_xgb, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sin busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""<strong>XGBoost con busqueda de hiperparametros</strong>"""

# Definir los hiperparámetros a ajustar

param_grid_xgb = {
    'learning_rate': [0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 1.0],
    'max_depth': [3, 5, 7, 9, 11],
    'n_estimators': [50, 100, 150, 200, 250],
}


# Inicializar GridSearchCV para XGBoost
grid_search_xgb = GridSearchCV(estimator=xgb.XGBClassifier(objective="binary:logistic", random_state=42),
                                param_grid=param_grid_xgb,
                                cv=kfold,
                                scoring='accuracy',
                                verbose=1,
                                n_jobs=-1)

# Entrenar el modelo utilizando GridSearchCV
grid_search_xgb.fit(X_train, y_train)

# Mejores hiperparámetros encontrados
best_params_xgb = grid_search_xgb.best_params_
print("Mejores parámetros:", best_params_xgb)

# Mejor modelo
best_model_xgb = grid_search_xgb.best_estimator_



# Realizar validación cruzada para accuracy
cv_accuracy_xgb_2 = cross_val_score(best_model_xgb, X_train, y_train, cv=kfold, scoring='accuracy')
cv_accuracy_xgb_2 = cv_accuracy_xgb_2.mean()

# Realizar validación cruzada para recall
y_pred_cv_xgb_2 = cross_val_predict(best_model_xgb, X_train, y_train, cv=kfold)
cv_recall_xgb_2 = recall_score(y_train, y_pred_cv_xgb_2, pos_label=1)

print("Accuracy promedio de validación cruzada: {:.2f}".format(cv_accuracy_xgb_2))
print("Recall promedio de validación cruzada: {:.2f}".format(cv_recall_xgb_2))

#Matriz de consufión para data train
y_train_pred_xgb_2 = best_model_xgb.predict(X_train)
conf_matrix_train_xgb_2 = confusion_matrix(y_train, y_train_pred_xgb_2)


# Reporte de clasificación
class_report_train_xgb_2 = classification_report(y_train, y_train_pred_xgb_2)
print('Reporte con datos de train:')
print(class_report_train_xgb_2)


# Crear Matriz
sns.heatmap(conf_matrix_train_xgb_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión con busqueda de hiperparametros')

# Mostrar la visualización
plt.show()

"""###**RESUMEN DE RESULTADOS DE LOS MODELOS USADOS**"""

# Lista de nombres de las predicciones sin búsqueda y con búsqueda

resumen = {
    'Modelo': ["Regresion logistica", "KNN", "Bayes", "Arbol de desicion ID3",
    "Arbol de desicion Cart", "Bagging", "Random forest", "XGBoost"],
    'CV Accuracy (sin búsqueda)': [cv_accuracy_rl, cv_accuracy_knn, cv_accuracy_nb, cv_accuracy_id3, cv_accuracy_cart, cv_accuracy_bag, cv_accuracy_rf, cv_accuracy_xgb],
    'CV Recall (sin búsqueda)': [cv_recall_rl, cv_recall_knn, cv_recall_nb, cv_recall_id3, cv_recall_cart, cv_recall_bag, cv_recall_rf, cv_recall_xgb],
    'CV Accuracy (con búsqueda)': [cv_accuracy_rl_2, cv_accuracy_knn_2, None, cv_accuracy_id3_2, cv_accuracy_cart_2, cv_accuracy_bag_2, cv_accuracy_rf_2, cv_accuracy_xgb_2],
    'CV Recall (con búsqueda)': [cv_recall_rl_2, cv_recall_knn_2, None, cv_recall_id3_2, cv_recall_cart_2, cv_recall_bag_2, cv_recall_rf_2, cv_recall_xgb_2]
}


# Crear el DataFrame
resumen = pd.DataFrame(resumen)
resumen

# Convertir los valores a dos decimales
resumen = resumen.round(2)
resumen = resumen.sort_values(by="CV Accuracy (con búsqueda)", ascending=False)

# Configuración para gráficas
plt.figure(figsize=(14, 12))
sns.set(style="whitegrid")

# Gráfica de Accuracy
plt.subplot(2, 1, 1)
resumen_melted_acc = pd.melt(resumen, id_vars=["Modelo"], value_vars=["CV Accuracy (sin búsqueda)", "CV Accuracy (con búsqueda)"], var_name="Tipo", value_name="Accuracy")
ax1 = sns.barplot(x="Modelo", y="Accuracy", hue="Tipo", data=resumen_melted_acc)
for p in ax1.patches:
    ax1.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 8), textcoords='offset points')
plt.title('Comparación de Accuracy entre Modelos')
plt.xlabel('Modelo')
plt.ylabel('Accuracy')
plt.ylim(0, 1.15)
plt.xticks(rotation=45)
ax1.legend(loc='upper right', bbox_to_anchor=(1, 1), ncol=1)  # Ajuste de la leyenda


resumen_r = resumen.sort_values(by="CV Recall (con búsqueda)", ascending=False)
# Gráfica de Recall
plt.subplot(2, 1, 2)
resumen_melted_recall = pd.melt(resumen_r, id_vars=["Modelo"], value_vars=["CV Recall (sin búsqueda)", "CV Recall (con búsqueda)"], var_name="Tipo", value_name="Recall")
ax2 = sns.barplot(x="Modelo", y="Recall", hue="Tipo", data=resumen_melted_recall)
for p in ax2.patches:
    ax2.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 8), textcoords='offset points')
plt.title('Comparación de Recall entre Modelos')
plt.xlabel('Modelo')
plt.ylabel('Recall')
plt.ylim(0, 1.1)
plt.xticks(rotation=45)
ax2.legend(loc='upper right', bbox_to_anchor=(1, 1), ncol=1)  # Ajuste de la leyenda

plt.tight_layout()
plt.show()

"""### **Conclusión:**
Basándonos en los resultados, el modelo de regresión logística  y XGBoost muestran un buen rendimiento en términos del equilibrio entre precisión y recall. La búsqueda de hiperparámetros parece mejorar ligeramente la precisión del modelo, lo que sugiere que ajustar los hiperparámetros puede ser beneficioso para mejorar los resultados, en comparación de lo demás modelos que se utilizaron.


Además recordando las matrices de confusión al correr cada modelo con el dataset de entrenamiento, se puede evidenciar que algunos modelos, al ser entrenados con este dataset, mostraron métricas de desempeño perfectas en el conjunto de entrenamiento, lo cual es un indicio claro de sobreajuste (overfitting). Este comportamiento se observó particularmente en modelos de árboles de decisión sin poda (es decir, donde no se buscaron hiperparámetros) y en ciertos modelos de ensamble. Aunque estos modelos lograban predicciones perfectas en los datos de entrenamiento, la validación cruzada reveló que métricas como el accuracy y el recall no eran óptimas en los conjuntos de validación. Igualmente estos modelos por sus métricas no califican para ser seleccionados.

## Usando los modelos seleccionados con los datos de test para medir su capacidad de generalización

### Validando Regresión Logística
"""

#Matriz de consufión para data test
y_test_pred_rl_2 = best_model_rl.predict(X_test)
conf_matrix_train_rl_2 = confusion_matrix(y_test, y_test_pred_rl_2)


# Reporte de clasificación
class_report_test_rl_2 = classification_report(y_test, y_test_pred_rl_2)
print('Reporte con datos de test:')
print(class_report_test_rl_2)


# Crear Matriz
sns.heatmap(conf_matrix_train_rl_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sobre test data')

# Mostrar la visualización
plt.show()

"""### Validando XGBoost"""

#Matriz de consufión para data test
y_test_pred_xgb_2 = best_model_xgb.predict(X_test)
conf_matrix_train_xgb_2 = confusion_matrix(y_test, y_test_pred_xgb_2)


# Reporte de clasificación
class_report_test_xgb_2 = classification_report(y_test, y_test_pred_xgb_2)
print('Reporte con datos de test:')
print(class_report_test_xgb_2)


# Crear Matriz
sns.heatmap(conf_matrix_train_xgb_2, annot=True, fmt='d', cmap='Blues', cbar=False)

# Configurar las etiquetas de los ejes y el título
plt.xlabel('Predicciones')
plt.ylabel('Realidad')
plt.title('Matriz de confusión sobre test data')

# Mostrar la visualización
plt.show()

"""## Conclusión final:
Al evaluar la capacidad de generalización de los modelos seleccionados, se evidencia que la Regresión Logística es el mejor modelo aplicable a este conjunto de datos, tanto por su buen accuracy como por su recall, que son las métricas claves del ejericio. El accuracy de Regresión Logística y XGBoost es igual (0.9); sin embargo, el recall para Regresión Logística es de 0.83 frente al 0.78 de XGBoost. Aunque ambos modelos demuestran una buena capacidad de generalización, para el propósito de este ejercicio se selecciona Regresión Logística.

# COMPONENTES DE ANÁLISIS PRINCIPALES (PCA)

El Análisis de Componentes Principales (PCA, por sus siglas en inglés) es una técnica de reducción de dimensionalidad ampliamente utilizada en la ciencia de datos y el análisis de datos. Su objetivo principal es transformar un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas, denominadas componentes principales.

El dataset de PCOS contiene muchas variables. PCA reduce el número de variables al encontrar nuevas variables no correlacionadas (componentes principales) que son combinaciones lineales de las originales y que explican la mayor parte de la varianza en los datos.

### Escalado de Datos

Primero, escalamos los datos utilizando StandardScaler de scikit-learn. Esto asegura que todas las características tengan una media de 0 y una desviación estándar de 1, lo cual es importante para PCA, ya que es sensible a las escalas de las variables.
"""

# Escalar los datos
scaler = StandardScaler(with_mean=True, with_std=True)
scaler.fit(df)

# Imprimir la media de cada característica
print(scaler.mean_)

# Transformar los datos escalados
df_std = scaler.transform(df)

"""Segundo, aplicaremos el Análisis de Componentes Principales (PCA) a los datos estandarizados. Utilizaremos la clase PCA de scikit-learn para ajustar el modelo PCA y transformar los datos."""

from sklearn.decomposition import PCA

# Aplicamos PCA
pca = PCA()
df_proyectado = pca.fit_transform(df_std)

"""Revisamos los componentes principales obtenidos del PCA. Los componentes principales son vectores que describen la dirección en el espacio original de las características en la que se encuentra la mayor varianza de los datos."""

# Obtener los componentes principales
print(pca.components_)

df.columns

print(pca.explained_variance_ratio_)

np.sum(pca.explained_variance_ratio_[0:28])

"""A pesar de tener muchas características (42), los primeros pocos componentes principales pueden capturar una proporción significativa de la varianza. Por ejemplo, con solo los primeros 28 componentes, capturamos alrededor del 88.9% de la varianza."""

# Varianza explicada por cada componente principal
var_exp = pca.explained_variance_ratio_

# Varianza acumulada
cum_var_exp = np.cumsum(var_exp)

# Visualización
plt.figure(figsize=(15, 7))
plt.bar(range(len(var_exp)), var_exp, alpha=0.3333, align='center', label='Varianza explicada por cada PC', color='g')
plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid', label='Varianza explicada acumulada')
plt.ylabel('Porcentaje de varianza explicada')
plt.xlabel('Componentes principales')
plt.legend(loc='best')
plt.title('Varianza explicada por cada componente principal y varianza acumulada')
plt.show()

"""## Conclusiones PCA

* El PCA ha permitido reducir la complejidad del dataset original, compuesto por 42 características, a un número menor de componentes principales que capturan la mayoría de la variabilidad en los datos. Esta reducción facilita la interpretación y el análisis posterior.

* Los primeros cinco componentes principales en conjunto explican aproximadamente el 31.82% de la varianza total.
Para explicar más del 50% de la varianza total, se necesitan aproximadamente los primeros 16 componentes principales.

* La varianza acumulada muestra que poco más de los primeros 20 componentes principales son necesarios para capturar aproximadamente el 80% de la varianza total. Esto indica que, aunque se puede reducir la dimensionalidad del dataset, se necesita un número considerable de componentes para mantener una cantidad significativa de información.

# Clustering
"""

df.describe()

"""## K-MEANS"""

from sklearn.cluster import KMeans

# Filtrar las variables eliminando las dicotómicas
variables_continuas = [var for var in df if '(s-n)' not in var]

# Crear un nuevo DataFrame con solo las variables numéricas continuas
df_continuo = df[variables_continuas]

# Escalar los datos
scaler = StandardScaler()
df_std = scaler.fit_transform(df_continuo)

# Inicializar K-Means
kmeans = KMeans(n_clusters=4, random_state=0)

# Aplicar K-Means
kmeans.fit(df_std)

# Obtener las etiquetas de cluster asignadas a cada muestra
labels = kmeans.labels_

# Obtener las coordenadas de los centroides de los clusters
centroids = kmeans.cluster_centers_

# Crear la gráfica de codo para determinar el número óptimo de clusters
WSSs = []
for i in range(1, 20):
    km = KMeans(n_clusters=i, random_state=0)
    km.fit(df_std)
    WSSs.append(km.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(range(1, 20), WSSs, marker='o')
plt.title('Gráfica de Codo')
plt.xlabel('Número de Clusters')
plt.ylabel('Suma de las distancias cuadráticas dentro del cluster (WSS)')
plt.show()

from sklearn.metrics import silhouette_samples, silhouette_score

k = 2


kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)
kmeans.fit(df_std)
y_clusters = kmeans.labels_
cluster_labels = np.unique(y_clusters)
silhouette_scores = silhouette_samples(df_std, y_clusters, metric='euclidean')

# Configurar la visualización del gráfico de silueta
y_ax_lower, y_ax_upper = 0, 0
yticks = []

# Iterar sobre los clusters
for i, c in enumerate(cluster_labels):
    silhouette_scores_c = silhouette_scores[y_clusters == c]
    silhouette_scores_c.sort()
    y_ax_upper += len(silhouette_scores_c)
    color = plt.cm.tab10(float(i) / k)  # Usar colores predeterminados
    plt.barh(range(y_ax_lower, y_ax_upper), silhouette_scores_c, height=1.0, edgecolor='none', color=color)
    yticks.append((y_ax_lower + y_ax_upper) / 2.)
    y_ax_lower += len(silhouette_scores_c)

# Calcular y graficar la línea de silueta promedio
silhouette_avg = np.mean(silhouette_scores)
plt.axvline(silhouette_avg, color="black", linestyle="--")

plt.yticks(yticks, cluster_labels + 1)
plt.ylabel('Cluster')
plt.xlabel('Coeficiente de silueta')
plt.title('Gráfico de Silueta')
plt.tight_layout()
plt.show()

"""Se exploraron diferentes métricas y técnicas para determinar el número óptimo de clusters, sin embargo, debido a la falta de un codo significativo en el gráfico de codo y la presencia de valores negativos en el coeficiente de silueta, se presenta un desafío en la identificación clara de la estructura de clusters en los datos.

El gráfico de codo se utilizó para identificar el número óptimo de clusters en el algoritmo K-Means. Sin embargo, la representación visual de la suma de las distancias cuadradas intra-cluster (WSS) en función del número de clusters no mostró un punto de inflexión claro o "codo". Esta falta de un punto de codo sugiere que no hay una división clara de los datos en un número específico de clusters.

El coeficiente de silueta se utilizó como una métrica alternativa para evaluar la cohesión y la separación de los clusters. Se observó que algunas muestras tenían valores negativos en el coeficiente de silueta, lo que indica que podrían estar más cerca del centroide de un cluster vecino que del centroide de su propio cluster asignado. Esto pueder ser una posible superposición o ambigüedad en la asignación de clusters.

## WARD (JERÁRQUICO)
"""

from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

Z = linkage(df_std, method='ward')

# Dendrograma
plt.figure(figsize=(12, 6))
dendrogram(Z)
plt.title('Dendrograma')
plt.xlabel('Índices de la muestra')
plt.ylabel('Distancia')
plt.show()

from scipy.cluster.hierarchy import dendrogram, linkage, inconsistent

# Calcular la inconsistencia
depth = 5  # Profundidad de cálculo de la inconsistencia
incons = inconsistent(Z, depth)

# Graficar la inconsistencia
plt.figure(figsize=(12, 6))
plt.plot(range(1, len(incons) + 1), incons[:, 2])
plt.title('Inconsistencia del dendrograma')
plt.xlabel('Número de fusiones')
plt.ylabel('Inconsistencia')
plt.show()

"""Se exploró la estructura de clustering mediante la construcción y análisis de un dendrograma. Basándonos en la observación del dendrograma, se sugiere que un número adecuado de clusters podría ser 5.

Análisis del Dendrograma:
Al observar el dendrograma generado, se puede notar que hay un punto en el cual los clusters se dividen en cuatro grupos distintos. Este punto se caracteriza por una fusión significativa en la altura de las ramas, indicando una potencial división natural del dataset en 5 clusters.

Los puntos del dataset se agrupan en cuatro clusters distintos, cada uno representando una subpoblación potencialmente diferente en el contexto del Síndrome de Ovarios Poliquísticos.

# 7. Bibliografía

- American College of Obstetricians and Gynecologists. (2015). Polycystic ovary syndrome. Obtenido el 20 de mayo de 2016 en http://www.acog.org/Patients/FAQs/Polycystic-Ovary-Syndrome-PCOS en el contenido de Inglés Notificaciόn de salida
- U.S. Department of Health and Human Services, Office on Women’s Health. (2014). Polycystic ovary syndrome (PCOS) fact sheet. Obtenido el 20 de mayo de 2016 en https://espanol.womenshealth.gov/a-z-topics/polycystic-ovary-syndrome
- FONTES, R.; et al. Reference interval of thyroid stimulating hormone and free thyroxine in a reference population over 60 years old and in very old subjects (over 80 years): comparison to young subjects. Thyroid Res. 6. 13, 2013
- WARD, L. S. Devemos mudar os valores de referência para TSH normal?. Arq Bras Endocrinol Metab. 52. 1; 2008

**Diseño asisitido por IA**
"""